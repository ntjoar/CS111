CS 111 Notes
Oracle Autonomous Linux (Sept 2019)
	No configuration
	Automatic patching, updating, and tuning
	No rebooting

OS is a tool for managing software and hardware
	They provide interface between programs and the kernel

$ls -l *bigFile*
							~10^19 bytes
-rw-r--r-- 1 eggert faculty 92233372036854775000 Sep 21 11:31 big
$grep x *bigFile*
$time grep x *bigFile*
real 0m0.009s<-10^(-2) seconds  == 10^(21)B/s
(Cir. 2008) hardware
^This machine is a dinosaur but bc it can read the metadata from a sparse file, it runs quickly

$grep -r /CIA/NSA/ 
^Will not find anything and run continually on our sparse file, but it doesn't know that because it cannot read the metadata properly
*Test this on SEASNet, but remove it the next day*
Sparse file is a type of computer file that attempts to use file system space more efficiently when the file itself is partially empty

31-Mostly idiots use github
32-Some non-CS majors use github work to check work
33-Many CS and non-CS majors github work to get it done
35L-All hail our lord and savior, github...

Grading
	20% Midterm (October 24) Open book open notes, Closed computer
	30% Final
	 5% Warmup assignment (Lab 0)
	40% 4 Labs (Each being 10%)
	 5% Report on current topics

Labs are going to be:
	Write a shell subset
	Synch
	File system
	TBD

Lateness (100 pts on assignment) <- Always turn in by 23:55
	1 day late: 1 point off
	2 day late: 2 point off
	3 day late: 4 point off
	...
	n day late: 2^(n-1) point off

Textbooks
	AD Renzi & Andre Arpoci-Dusseu
	   OSTEP: CS: Three easy pieces
	SK Salter & Kaashook
	   Principles of Computer System Design 
	Other reading:
		M. Kampe

For Tuesday:
	SD Chap 1-2, 36
	SK 1, 2-2.3
	Kampe on interfaces

Good citations ==> Vancouver System is good, URIs, DOIs
	Author
	Name of work
	Journal
	Date
	Vol #

System
	OED "System" (1928)
		i.  an organized or connected group of objects
		ii. a set of principles etc., a scheme, method
	From Greek (σψστεμα) 
		Organized, whole, government, constitution, ...
	SK: A system is a set of interconnected components that has specified behavior observed at the interface with its environment

Operating system
	American heritage 4th ed (2000)
		Software to control the hardware of specific data processing system in order to allow users and application progams to make use of it
		Key: Software controls hardware

Encarta 2007
	Master control program in a computer

Wikipedia v. 917297650 (2019-09-23)
	System software that manages computer hardware and software resources and provides common services for computer programs

OS Goals: (Last two goals are internal goals)
	Protection
	Robustness
	Utilization (Trying to make effective use of your system)
	Performance (Time, memory, energy, ...)
	Simplicity (Not being linux)
	Flexibility

Booting (Means to the goal, not the actual goal)

Tradeoffs: "Waterbed effects"
	Security and performance
	Time and memory
	We always have a tradeoff
Incommesurate Scaling
	Not all parts grow at the same rate
	Economies of scale (Pin factory)
	Diseconomies of scale (Star network)
Emergent properties
	You get effects as your system scales up that you don't expect
Propogation of effects
	One change can change the rest of the modules connected and break later on

Our operating systems are too complex 

DISCUSSION

Lab 0 due next wednesday, 10/02
O_TRUNC flag to truncate to 0
man 2 open -> open(2) man page
while(ret = read(fd, buf, buffer(size)) > 0) 
	print ret

reserved file descriptors 
0 - STDIN 
1 - STDOUT
2 - STDERR
3 - "/tmp/foo.txt"
4 - "path" 
..
close(3) -> 
0, 1, 2, 4 

open(...) -> fd 3 used, not 5

Takes smallest integer available
--
--
inputfd = STDIN 
outputfd = STDOUT

if inputfile is provided 
    set input fd to be open(inputfile)

if outputfile is provided 
    set output fd to be open(outputfile)

STDIN = 0
STDOUT = 1
--segfault 
Write code that is bad 
store var in null ptr 
free memory and then use ptr 

--catch
SIGSEGV - code that is waste, captures segfault, logs msg on stderr and exit(4)
if there is a segfault, log in on stderr 

signal(SIGSEGV, signal_handler)
function signal_handler()
    print error message 
    exit(4)

How to read arguments?
argv and argc passed into main 
int main(int argc, char* argv[])


SHOULD STILL WORK - dont hardcode 
./lab0 --input=foo --output=bar
./lab0 --output=bar --input=foo 

getopt -> parses string for us

--input=val is the same as -i=val (args)
--segfault is the same as -s (boolean)

open() - how to know if permissions bad, or file DNE? 
strerror() - interprets error
errno - number of last error, read open man
    2 - no such file or dir 
    3 no such process 
    13 permissions denied

if open returns -1, 
strerror returns a ptr to a string that describes error code 

fd = open(...)
if(fd == -1)
{
    printf(strerror(errno));
    exit(code )
}

Error codes:
0 ... copy successful
1 ... unrecognized argument
2 ... unable to open input file
3 ... unable to open output file
4 ... caught and received SIGSEGV
139 ... dumped core

Makefiles 
- name: Makefile 

format:
target : dependency...
[TAB] : actions 
[TAB] : actions

default : lab0.c
    gcc ..
check : 
    #run default 
    #run test
clean :
    rm -rf files
dist : 
    tar ... 

? need PHONY ?

smoketest: sanity test
    tests bare minimum of program 

2 gdb screenshots >:(
compile with g flag, for gdb 
gdb lab0 
break filename:linenumber
segfault:910, break lab0:910
print var(variable in lab0)
start debugger 

2 SCREENSHOTS
set breakpoint to null thing
print value of NULL PTR, run it again, show fail 
2 screenshots

---

AD Chapter 1-2, 36
Virtualization, Concurrency and Persistence
How OS works
	What program it decides to run next on CPU
	How it handles memory overload in a virtual memory system
	How VM monitors work
	Manage info on disks
	Distributed systems
Processor
	Fetches->Decodes->Executes
	This happens for each instruction
		This is the Von Neumann model of computing
		Von Neuman goal is to make the system easy to use
Body of the software is responsible to make the program easy to run programs
	Includes memory handling, enabling interactions and othher fun stuff
	This is what is known as OS
Primary way that OS makes other software easy to run is through virtualization
	Take physical resource and transform it to an easy-to-use virtual form of itself
OS provides APIs, which are easy to call features of the virtual machhine
	A typical OS has a few hundred of these system calls
	OS provides calls to run programs, access memory -> standard library to applications
Virtualization  allows many things to run, so OS needs to manage resources
Virtualizing the CPU helps turn a single CPU with one core into seemingly infinite cores
Policy of OS chooses which program to run first
Memory is an array  of bytes
To read memory, one must specify address to be able to access data stored there
	Similarly, writing must have a specified address
PID is the process identifier
Virtualization allows each process to be updated somewhat simultaneously
Main program creates two theads when you call pthread_create()
Thread is a function running witin the same memory space as other functions
DRAM stores values in a volatile manner -> It goes away with power and with crashes
Hard drive is a common repository for long-lived info
Disk OS is usually called the file system
	It needs to store files in a reliable and efficient manner for the system
OS does not create private, virtualized disks per application
System calls are routed to the file  system, which handles requests and returns some kind of error code to theh user
Abstractions help the system easy and convenient to use
OS goal is to provide virtualization efficiently without much overhead
Protection is another goal of the system, which incorporates much isolation
Energy-efficiency, security and mobility are also important
OS started as standard libraries
Batch processing - set up a number of jobs and run together
File systems - developed in order to stop applications from taking from each other's files
System call vs procedure call
	Sys calls transfers controls and raises hardware privilege level
		Hardware restricts what user mode apps can and cannot do
		Sys calls makes hardware to raise control to a trap handler and raises privilege level to kernel mode
			Here it can control full access to the hhardware
		When done, all control is passed to the user
I/Os are the only way a computer works
Faster busses are shorter
	Decreases cost and increases effectivity
The first component of any device is the interface that it provides to its system
	How does the hardware control its parts
Internal structure is the second part
	Is implementation and abstraction device allows for the system
Status register
	Checks status of device
Command register
	Tells device to do a command
Data register
	Essentially passes data to and from the device
OS always waits until device is not busy and polls the device (reading the status register)
Writes data to data register
Writes command to register  -> Starts device and executes the command
Wait until device is done with your request
^Basic protocol is slow however
Interrupt is when OS issues request and puts current calling process to sleep and switch to another task
Interrupt handlers pause a process and wake process due to an interrupt
Interrupts allow for overlap in computation and I/O
Coalescing allows for CPUs control when to use an interrupt an application requests
	This allows for CPUs to finish other processes that may be more efficient
Direct memory access is essentially caching data
	OS programs the DMA to directly send memory in x address at z bits to y 
Explicit I/O instructions allow OS to send data to specific registers and allow construction of protocols described above
	x86 is an example of this
	Such interactions are privileged, meaning OS is only handler of such memory
Memory mapped I/O, where hardware makes device registers available as if they were memory locations
	To read OS issues a load (read) or store (write) the address
Abstraction in I/O -> Software in OS must know how device works
	Device driver
Raw interface -> allows special apps to directly r/w blocks w/o file abstraction usage

SK 1, 2-2.3
Problems in systems are divided into 
	Emergent properties
		Properties not evident in individual components but show up when combining
	Propogation of effect
		Basically the butterfly effect
		When one thing is connected and relied on, a bad update can actually ruin a different system
	Incommesurate scaling
		As a system increases in size or speed, not all parts agree, so things stop working
	Tradeoffs
		If you get some goodness, and use it up, allocating it to where you need it the most, you will still have to sacrifice
		Waterbed effect
			Pushing down one end, raises the other
		Binary classifcation - classifying into two things, like gender
			We identify and use some indirect measure to divide, bc of a lack of direct measure due to a property falling somewhat in between <- Proxy
System is a complex unity formed of diverse parts (components) subject to a common plan or serving a common purpose (interconnections)
A system is a set of interconnected components that has an expected behavior observed at the interface with its environment
Ability to choose granularity means that one component may be its own system
	We define thhis as a subsystem
Complex is difficult to understand
	This is subjective and relative
Signs of complexity
	Large number of components - Sheer size affects our POV on component complexity
	Large number of interconnections - A few components can be interconnected in a large number of ways
	Many irregularities - Lack of consistency from component to component
	A long description 0 Large descriptions can be daunting
	A team of designers, implementers, or maintainers - Many people help maintain it and are required to understand the various parts of it
Abstraction is to limit the depth at which we see complexity
Sources of complexity
	Cascading and interacting Requirements
		Adding requirements adds complexity
		Accumulations can cause intertwining complexity
		Rises from pressure for generality and exceptions that add complications
		Can lead to Emergent Properties and prop of effects
		Principle of escelating complexity
			Adding a requirement increases complexity out of proportion
		Avoid excessive generality
			If it is good for everything, it is good for nothing
		First response is to make patches
	Maintaining High Utilization
		One requirement by itself for complexity is high performance or high efficiency
		Whenever a scarce resource is available, we aim to keep utilization high
		Law of diminishing returns
			The more one improves some measure of goodness, the more effort the next improvement will require.
Coping with Complexity
	Modularity
		Divide and conquer
		We seperate the systems into respective subsystems and call them modules
		This helps us think of components by themselves before they interconnect
		Unfortunately time grows proportional to square of size
			Modularization divides time by a factor of K modules
		The unyielding foundations rule
			It is easier to change a module than to change the modularity
	Abstraction
		Although modularization is good, it may suffer from propogation of effects
		This is an additional requirement on modularity
		Abstraction is seperation is seperation of interface from internals - specifcation from implementation
		State is abstracted into particular, easy-to-describe modules called registers
		Minimizing interconnections may fail if accidental or unintended interconnections occur due to implementation errors
		The robustness principle
			Be tolerant of imputs and strict on outputs
		The safety margin principle
			Keep track of the distance to the cliff, or you may fall over the edge
		Shake-out - modules check every input carefully and refuse to accept out ofspecification
		Production - Modules accept any input that they can reasonably interpret in accordance with the robustness principle
	Layering
		Building on top of mechanisms that is already complete
		Module of a given layer should only interact to that which layers it is connected to 
		Computer systems (Bottom up)
			Gates and memory cells
			Processor and Memory
			OS
			Application
	Hierarchy
		Starting with a group of small modules and assembling them into a larger subsystem
			Repeat for other modules
			Continue until you can assemble the system from all the rest of the subsystems
		Results is a tree like structure
		Similarly to how a corporation is setup
		Components should interact only to which it is connected to, not in its own layer
	Names end up connecting all of the above modules
	Decouple modules with indirection
		Indirection supports replacability
	Binding - When the designer chooses a specific implementation from among many tat are available
	Delay binding by  naming features than implementing it
		Helps plan ahead
Computers Are DIFFERENT
	Computer Systems have nearly no bounds on composition
		Analog systems are limited in that each component contributes noise
			More complex = more noise
			Noise limits number of analog components tat a designer can usefully compose
		Digital systems are noise-free
			There is no constraint here
			Static discipline - requires that the range of analog values a device accepts as meaning as the digital value ONE (or ZERO) be wider than  range that it puts out
		Digital systems are constructed of analog components
			However, components chosen are nonlinear and allows a wide tolerance
		Regenerated levels create clean interfaces that allow one subsystem to be connected with the next
		There is no guarantee that devices don't fail
		Digital systems can grow beyond our own understanding even though we made them
	d(Technolgy)/dt is unprecedented
		Incommesurate scaling rule
			Changing any system parameter by a factor of 10 usually requires a new design
		Designers must adjust for strains of system change
		First drafts always change
Coping with Complexity II
	In the real, fast-changing world of computer systems, it is hard to choose
		the right modularity from a sea of plausible alternative modularities
		the right abstraction from a sea of plausible alternative abstractions
		the right layering from a sea of plausible alternative layerings
		the right hierarchy from a sea of plausible alternative hierarchies
	Iteration
		Start by building a simple working system that meets only a modest subset of the requirements and then evolve that syste in small steps to gradually encompass more and more of the full set of requirements
		Small steps can help reduce of overcomplicating
		Design for iteration
			You won't make it right the first time, so make it easy to change
		Take small steps
			Allow discovery of design mistakes and bad ideas quickly so they can be changed or removed with little effort before other parts cause issues
		Don't rush
			Carefully plan it
		Plan for feedback
			Just like the resume
		Study failures
			Complex systems fail for complex systems
	Keep it simple
		Problems
			previous systems give a taste of how great things could be if more features were added
			the technology has improved so much that cost and performance are not constraints
			each of the suggested new features has been successfully demonstrated some- where
			none of the exceptions or other complications seems by itself to be especially hard to deal with
			there is fear that a competitor will market a system that has even more features
			among system designers, arrogance, pride, and overconfidence are more com- mon than clear awareness of the dangers of complexity
		Adopt sweeping simplifications
			So you can see what you are doing
Three fundamental abstractions
	Memory
		Storage, system component that remembers values for use in computation
		write(name, value)
		value <- read(name)
		write operation specifies in value a value to be remembered and in name a name by which one can recall that value
		Read specifes in name the name of some prev remembered value
		Memory can be volatile or non-volatile
			Volatile  -> mechanism of retaining memory requires energy (me during any class)
			Durable -> Designed to remember for a specified period
			Non-volatile memory forgets over time (decay)
		Hardware memory devices: RAM chip
			Flash memory
			Magnetic tape
			Magnetic disk
			CD-R and DVD-R
		Higher level memory systems:
			RAID
			File system
			Database management system
		Read/Write Coherence means that result of read of a named cell is always the same as the most recent write to that cell
		Before-or-after atomictiy means that result of read or write is as if read or write occurred either before or completely after any other read or write
		There are threats
			Concurrency
				During multithreading, collisions can happen
			Remote storage
				This happens during remote storage, wherein we ask which write was the most recent
			Performance enhancements
				Compilers may decide to hold read/writes such that it leads to undefined results
			Cell size incommesurate with value size
				RAM unavailability
			Replicated storage
				Lost track of a certain value bc it is copied over a few cells
		Memory Latency
			It takes time for an operation to be completed
			Due to piling up processes, it can bottleneck here
			RAM is one where latency for mem cells is chosen at random
			Electronic memory chip is usually configured for random access
		Memory names and addresses
			Physical implementations of mem devices nearly always name a memory cell
			Integer names are known as the address and they form the address space of a memory
			Think of this like a grid
		RAID
			Redundant array of independent disks
				Consists a set of disk drives and controller configured with electrical and programming interface similar to single disk
			RAID improves performance through read and write concurrently 
				Also durability by having redundancies duh
			RAID Types
				0 increaces concurrent reading and writing
				4 improves disk reliability via error-correction codes
				1 gives high durability through cloning disks
				5 gives simple but elegant performance optimization
	Interpreters
		Active elements of a computer system
		Perform the actions that constitute computations
		Abstractions
			An instruction reference, which tells the interpreter where to find its next instruction
			A repertoire, which defines the set of actions the interpreter is prepared to perform when it retrieves an instruction from the location named by the instruction reference
			An environment reference, which tells the interpreter where to find its environment, the current state on which the interpreter should perform the action of the current instruction
		Hardware:
			Pentium 4, PowerPC 970, UltraSPARC T1 disk controller
			display controller
		Software:
			Alice, AppleScript, Perl, Tcl, Scheme LISP, Python, Forth, Java bytecode JavaScript, Smalltalk
			TeX, LaTeX
			Safari, Internet Explorer, Firefox
		Interrupts can catch the eye of an interpreter and force it to take control of the program
			Essentially program controls the interpreter until otherwise said
		Usually asynchronous
		Processor
			Implementation of an interpreter
			Instruction reference is a program counter
			Processor refers to memory indirectly, often through registers
		Interpreters are organized in layers
			Lowest is hardware engine that has simple repertoire of instructions
				Higher you get, the richer it gets
			Example of layered design
			Higer ups expect lower levels to do their tasks properly
		Communication Link
			How to move in between physically seperated components
			send (link_name, outgoing_message_buffer) 
			receive (link_name, incoming_message_buffer)
				Send op specifies an array of bits called a message to be sent over the link identified by link_name
				The argument outgoing_message_buffer identifies the message to be sent
				receive operation accepts an incoming message, again usually by designating the address and size of a buffer in memory to hold the incoming message
				Once the lowest layer of a system has received a message, higher layers may acquire the message by calling a receive interface of the lower layer, or the lower layer may “upcall” to the higher layer, in which case the interface might be better characterized as deliver (incoming_message)
			Hardware technology: 
				twisted pair
				coaxial cable optical fiber
				Higher level Ethernet
				Universal Serial Bus (USB) the Internet
				the telephone system
				a unix pipe
Naming in Computer systems
	Comp sys use names in many ways in their construction, configuration, and operations
	In a direct analogy with two ways in which procedures can pass arguments, there are two ways to arrange for one object to use another as a component:
		create a copy of the component object and include the copy in the using object (use by value), or
		choose a name for the component object and include just that name in the using object (use by reference). The component object is said to export the name
	When passing arguments to procedures, use by value enhances modularity, because if the callee accidentally modifies the argument it does not affect the original
	Decoup;ing one object from another by using a name as an intermediary is known as indirection
	Deciding on the correspondence between a name and an object is an example of binding
	Naming scheme
		Name space, alphabet of symbols together with syntax rules that specify which names are acceptable
		Name-mapping algorithm, which associates some names of the name space with some values in a univers of values
		Universe of values - some cluster of values in a program
	Interpreters must figure out what naming scheme is involved and whhich version of a command it must invoke
	status ← bind (name, value, context) 
	status ← unbind (name, context) 
	list ← enumerate (context)
	result ← compare (name1, name2)
		The first operation changes context by adding a new binding
		the status result reports whether or not the change succeeded (it might fail if the proposed name violates the syntax rules of the name space)
		After a successful call to bind, resolve will return the new value for name
		The second operation, unbind, removes an existing binding from context, with status again reporting success or failure (perhaps because there was no such existing binding) 
		After a successful call to unbind, resolve will no longer return that value for name
		The bind and unbind operations allow the use of names to make connections between objects and change those connections later
	Different naming schemes have different rules about the uniqueness of name-to- value mappings
	Another kind of uniqueness rule is that of a unique identifier name space, which provides a set of names that will never be reused for the lifetime of the name space and, once bound, will always remain bound to the same value
	A possible outcome of performing resolve can be a not-found result, which resolve may communicate to the caller either as a reserved value or as an exception
	Finally, some naming schemes provide reverse lookup, which means that a caller can supply a value as an argument to the name-mapping algorithm, and find out what name or names are bound to that value
	In practice, one encounters three frequently used name-mapping algorithms:
		Table lookup
		Recursive lookup
		Multiple lookup
	A default context reference is one that the resolver supplies
	an explicit context reference is one that comes packaged with the name using object
	Context references for names found in an object
		Default: supplied by the resolver 
			Constant built in to the resolver
			Variable from the current environment 
		Explicit: supplied by the object
			Per object
			Per name (qualified name)
	The working directory acts as the default context for resolving file names
	In contrast, an explicit context reference commonly comes in one of two forms: 
		a single-context reference intended to be used for all the names that an object uses
		a distinct context reference associated with each name in the object
			The second form, in which each name is packaged with its own context reference, is known as a qualified name
	A context reference is itself a name (it names the context), which leads some writers to describe it as a base name
	In a typical design, the resolver uses one of two default context references:
		A special context reference, known as the root, that is built in to the resolver
		The path name of yet another default context
	Path names can also be thought of as identifying objects that are organized in what is called a naming network
	indirect name
		symbolic link, soft link, alias, and shortcut
	multiple lookup is to abandon the notion of a single, default context and instead resolve the name by systematically trying several different contexts
	A common such scheme is called the search path, which is nothing more than a specific list of contexts to be tried, in order
	By placing a library that contains personally supplied programs early in the search path, an individual user can effectively replace a library program with another that has the same name,thereby providing a user-dependent binding
	A path name is a name that carries its own explicit context, while a search path is a context that consists of a list of contexts
	3 way handshake -> split-transaction through a bus
		Interrupt handler struggles with this one^
		DMA helps
	A file has two key properties:
		It is durable
		 	Information, once stored, will remain intact through system shut- downs and can be retrieved later, perhaps weeks or months later
		 It has a name
		 	The name of a file allows users and programs to store information in such a way that they can find and use it again at a later time

Kampe Software Interface Standards
Software standards started with the fact
	New applications have to work on both old and new devices <- Apple apparently didn't get the memo
	Software upgrades (e.g. to the underlying operating system) cannot break existing applications. <- Again, apple did not get the memo
Standards pros
	Details are discussed by experts all the time
	Platform neutral, so it isn't biased towards a contributor
	Give clear and complete specifications
	Allows for freedom to provide uniqueness as long as they serve the current purpose
...However
	It constrains implementations
		When a new method could improve implementations but are not compatible with old methods
	Interface standards impose their own constraints on their consumers
		Nonstandard interfaces are more likely to have people not exactly understand how to use it, even though it may be better
	Stake-holders depend on a standard
		Competing opinions
		Any change adversely affects at least one person
Interface standards specify behavior
Ture implementation neutrality is difficult
Since implementation was not developed to or tested against spec, it may not comply
In absence of spec, it may  be difficult to determine whether a behavior is fundamental in the interface
Technology in computers are always evolving
Types of apps are also evolving, but people still think it's possible to run apps on a abascus for whatever reason
Maintaining stable interfaces is difficult
	We are forced to choose between
		Staying where we are to accomodate for previous products
		Moving forward but leaving standards
		Comprimise, but this rarely works
Proprietary interface <- one that is developed and controlled by a  single organization
	Difficult to keep up
	Competing standards fragment the market
	Shouldering full risks of dev
Open standard <- maintained by a consortium of providers/consumers
	Reduced freedom
	Forced re-engineering
	Giving up competitive edge
Typical API
	A list of included routines/methods, and their signatures (parameters, return types)
	A list of associated macros, data types, and data structures
	A discussion of the semantics of each operation, and how it should be used
	A discussion of all of the options, what each does, and how it should be used
	A discussion of return values and possible errors
API specs are written at the source programming level
	Describe how source code should be written
	Developers  to a particular API can easily recompile and execute that API
	Any apps using that API will port to the platform well
	64-bit API may not work on a 32-bit machine
	API accessign individual bytes within an int may not work in big-endian
	APIs assuming particular feature is that is may not be universally applicable
	Open standard helps alleviate thhis issue
Application Binary interfaces (ABI)
	the binding of an Application Programming Interface (API) to an Instruction Set Architecture (ISA)
	An API defines subroutines, what they do, and how to use them
	An ABI describes the machine language instructions and conventions that are used (on a particular platform) to call routines
		The binary representation of key data types
		The instructions to call to and return from a subroutine
		Stack-frame structure and respective responsibilities of the caller and callee
		How parameters are passed and return values are exchanged
		Register conventions (which are used for what, which must be saved and restored)
		The analogous conventions for system calls, and signal deliveries
		The formats of load modules, shared objects, and dynamically loaded libraries
	ABI is more portable
	An app written by an API and compiled and linked by ABI should run on any platform supporting ABI
Most programmers will never directly deal with an ABI, as it is primarily used by:
	the compiler, to generate code for procedure entry, exit, and calls
	the linkage editor, to create load modules
	the program loader, to read load modules into memory
	the operating system (and low level libraries) to process system calls

---

Complexity
	Hardware Complexity: Moore's Law
		Number of transistors on a chip should double every 18 months
	#Transistors/Chip
		Now, there are 10^10
		In 1970, there were 2x10^4
	Cryder's Law
		Moore's law for Hard drives
		1970: 1GB
		Now: 10TB
	We grow harware exponentially, but not speed 

AD: main problems in OS
	Virtualiztion
	Concurrency
	Persistence
	Prof Eggert adds
		Evolution
		Flexibility
		This is a cause of us being part of the equation
			Eggert likes the side of having standardization^

AD: Tools for attacking these problems
	Abstraction & Modularity
		Think divide and conquer
		Distinction between interface and implementation
			Interface - dividing lines between modules
			Implementation - How each module works
		Policy vs Mechanism
			Policy - high-level decisions about an OS
			Mechanism - What instructions actually run
			Keep these seperate
				i.e. implement policy to different mechanisms and vice versa
	Measurement and monitoring
		No system runs perfectly, adapt to the system, not the other way around
		This covers performance and correctness

Evolution and flexibility
	Interface evolution
		Expanded upon based off AD tools for attacking problems in OS

Standalone (bare-metal) app for paranoid professors
	Backstory: Academia professor wants to put a grant proposal into a machine, but doesn't want it stolen by a hacker or competition
	Prof uses a desktop (unnetworked) in the center of a room, literally away from everything in the room
	Write an app that essentially runs the same thing as wc in linux
	Input: It is an ASCII text file
		Def of a word is Regex: [A-Za-z]
	Output: 35721
	User runs the program by powering the computer on
	When done with the number, power the computer off
	Hardware
		Intel Core i3-4460
		3MiBL3 Cache
		3.6GHz 
		4GiB dual channel DDR3SDRAM 600MHz
		1TB SATA hard drive 7200 RPM
		Intel HD4400 Graphic 
	File starts at block 100K
		File ends with null byte (guaranteed)
	Use 512 byte sectors

x86 boot procedure
	1MiB Physical RAM 
	At the end is the Basic Input-Output System(BIOS) Code
		Read-only memory (ROM)
			Normally EEPROM
		Supplied typically by the motherboard manufacturer
	Anything before BIOS starts with all 0s
		This is all DRAM
			Volatile, so useless when we start up
BIOS 
	Tests the system first
	Looks for devices via IO instructions
		Checks for what is attached to it (on the bus)
	Looks for a device that contains the Master Boot Record (MBR)
	Sometimes touches RAM looking for tests
	Runs a jump instruction to set %rip to 7c00
	Reads MBR into location 0x7c00

Classic MBR layout
	Imagine a 512-byte spread
	The first 486-bytes are for your code
	2-byte ending are the signature 
		0x55 0xAA
		0xAA55 <- note little endianness
	64 bytes preceding final 2 are seperated into four partitions
		Contains status
			Is it bootable
		Contains starting location
			Which sector number is used
		Size in sectors
	MBRs look through partitions find the first bootable one and copies it into RAM and repeats until all are booted
		Chain loading
			Why? Because it defers the job of booting to files that know how to boot it
				Just like calling a plumber to fix pipes
	x86 Code (486-bytes) | 4 partition entries (64-bytes) | Signature (2-bytes)

Process of Boot
BIOS -> MBR -> VBR -> GRUB -> Linux Kernel ->
					  Grand Unified Boot Loader

GRUB is like an OS that boots other OS... Among other things

For our solution
BIOS -> WC
gcc -m32 -> Compiles for the x86, not x86-64

read-sector.c
void read_ide_sector(int s, char *a) { // Read this part through a technique known as Programmed I/O (PIO)
	while((inb(0x1f7) & 0xc0) != 0x40) // 0x1f7 is the status register
		continue;
	outb(0x1f2, 1);
	outb(0x1f3, s & 0xff);
	outb(0x1f4, (s>>8) & 0xff);
	outb(0x1f5, (s>>16) & 0xff);
	outb(0x1f6, (s>>24) & 0xff);
	outb(0x1f7, 0x20); // Read sectors command
	while((inb(0x1f7) & 0xc0) != 0x40)
		continue;
	insl(0x1f0, a, 128); // 128 is for Words (32-bits)
}

bool isalpha(int x) {
	return A<=x && z >= x;
}

void display(long long nwords) {
	do {
		unsigned char *screen = 0xb8000 (4000/2) + 60;
		screen[0] = nwords % 10 + '0';
		screen[1] = 7; // grey on black
		screen -= 2;
		nwords /= 10;
	} while (nwords != 0);
}

void main(void) {
	long long nwords = 0; // must be long long so it can be big enough to count
	bool in_word = false;
	/* We start reading at sector 100,000 */
	for(int s = 100000;; s++) {
		char buf[512];
		read_ide_sector(s, buf);
		for(int i = 0; i < 512; i++){
			if(buf[i] == 0) {
				nwords += in_word;
				display(nwords);
				while(true) // Purposeful inf loop. Prof needs to turn off to stop the  loop
					continue;
			}
			bool is_alpha = isalpha((unsigned char)buf[i]);
			nwords += in_word & ~is_alpha
		}
		in_word = is_alpha;
	}
}

-> unsigned char inb(int addr) {
	asm("inb___");
}
s is the sector Number
a is  mem address of read buffer
inb -> read sector
outb -> write sector
insl -> similar to inb, but takes disk controller data and copies to RAM
	Disk controller doesn't know it has RAM. It just carries out the request it gets

0x1f7 is the status register
0x1f2 is the count register

Read about disk controller
Status register
	First two bits if 0 and 1 in that order, the device  is ready

void outb(int a, unsigned char data){
	asm(...)
}

You must run gcc to build and compile on your own machine

Memory-Mapped I/O
	Runs load and stores

Display Buffer
	Low order bytes is char
	High order is attributes

Problems with standalone app
	1) duplication of code
		Read IDE sector does the same thing that the BIOS does
	2) Chewing up the bus with insl
		It ties up the bus by sending data to CPU so it can be forwarded to RAM
		Nowadays, there is DMA
			It sends command to disk controller still, but it just tells for the data to be sent to RAM
	3) Special purpose app, not easily genreralizeable to different hardware
		UEFI Booting -> not gonna work
	4) This program chews up the CPU
		Code focuses on waiting and just using up the CPU
		Not very efficient
		Run the word count program at the same time as other programs
		We can complicate our code
			continue->yield()
	5) Hard to reuse pieces in other apps
		Recompilation, copying, etc.
	6) If the application goes wrong, yer done fucked
		No recovery from this
		Seg faults must suck
		We are CS, we must strive to look beautiful, unlike poly sci... and gender studies

Avoiding OS
Painful to change programs
Painful to reuse parts of programs
	We will have to cut and paste or the equivilent thereof
Hard to run programs concurrently
	Remember from reading that OS handles all of this
Hard to recover from faults
	
OS is normally the part that handles this problem

Coping with Complexity
Modularity
	Break your program into pieces with interfaces
Abstraction
	Basic idea is that you have good interfaces
	Should be as clean, simple, and good as possible

Arg for modularity
Assume N lines of code, K modules
	One big sad program
		Assumes bugs ocN cost of fixing a bug is proportional to N
		Debug time is proportional to bugs times cost proportional to N^2
	K modules
		Cost is proportional to N^2/K
		Forgets to account for modules affecting each other
void read_ide_sector(int s, char *a);  // Assumes 512-byte reads
void read_sector(int s, char *a, int sectsize);
int read_sector(int s, char *a, int sectsize); // Returns error code
int read_sector(int s, char *a, int bufsize); // Has nothing to do with the device

^ Starting to look like
	ssize_t read(int fd, char *a, size_t bufsize); 
	int goes to 2^(31)-1
	size_t goes to 2^(64)-1
	ssize_t is  more flexible than int
	fd allows you to read from more sectors than just RAM devices
	The other interfaces are for a single device, this can be used to apply to different devices
	API selects device for each call
	ssize is signed size

int fd=open("foo.txt", O_RDONLY);
	Mutate this to support the networked files
		remote=open("lnxsrv06.seas.ucla.edu", "foo.txt", O_RDONLY);
			This we  would  have to mutate code to adjust user permission
	open("lnxsrv06.seas.ucla.edu:foo.txt", O_RDONLY);
		This will straight change this to $cat lnxsrv06:/etc/passwd
		: means please look
	Instead, we don't make special punctuation and do
		open("//remote/lnxsrv06.seas.ucla.edu/foo.txt", O_RDONLY);

Suppose you write a code for reading lines from file descriptors
	char *readline(int fd);
		Problems:
			Assumes system call will do memory allocation
				Memory can be fully used and will return a NULL Ptr, so what do we do with what we have?
			char *readline(int fd, char *buf, size_t bufsize);
				Application must specify a line length limit
			char *readline(int fd, char **buf, size_t *bufsize);
				What happens when bufsize runs out?
					API tries to reallocate but fails so segfault

Good vs bad modularity: some metrics
	Performance
		You want an interface or a set of interfaces that helps you run it properly
			Modularity hurts performance
				Think building with more walls and doors
	Robustness
		Should be tolerant of faults int  the system
		Single failure should not fail it
	Simplicity
		Make it user friendly
		No one likes learning something entirely foreign to them, just like why no one likes verilog
	Neutrality
		Make it universal
		Make as few assumptions as possible
	Evolvability
		Keeps the money flowing in
		Also you want to keep up with the times

$cat >fact.c <<EOF
	int factorial(int n) { // what if n<0?
		if(n<0) return -1; // I would assume this is necessary
		return (n==0 ? 1:n*fact(n-1));
	}
ECF
$gcc -S -C1 fact.c
$cat fact.s

arg is in edi
fact:
	movl $1, %eax
	testl %edi, %edi <-n==0
	jne .L8
	ret
.L8:
	pushq %rbx
	movl %edi, %ebx
	leal -1(%rdi), edi
	call fact
	imult %ebx, %eax
	popq %rbx
	ret

callee messes with caller
	Caller is not insulated from callee's fault
		Same with vice versa
	Only works if both sides trust and can work with one another
badfact1:
	movq $0x39c54e16, 16(%rsp)
badfact2:
	jmp badfact2

Function calls are soft modularity
	You must trust and the caller and callee and they must trust each other
		Cheap but unsafe, just like prostitutes
We want hard modularity
	No trust needed
	One or both modules should be protected
3 Fundamental System abstractions for hard modularity
	I) Interpreter
		Run the untrusted module in a software "machine" 
			Example is JVM (Java Virtual Machine)
		Downside is that it is slow as fuck
	II) Virtualization 
		Run untrusted module in a fake machine
		Check as it runs
		Enlists hardware support to make our interpreter fast
		This is a hardware support for interpreters
		However, we must run instructions compatible for the run code
	III) Client/Server
		Hold the plague at bay and run it on a seperate piece of hardware
		The little shit can't affect your machine immediately
		Communications will be through a message
		Think quarantine zones for software codes
			If only this existed for people and I could legally do this to people as well
	I and II require shared mem
	III requires network messages

Virtualization runs it on the same piece of hardware
	It requires hardware support to protect the rest of the applications from the untrusted one
	At a hardware level, we have HW interfaces that contains instructions
	Our untrusted code goes on a different level and sits most of its code on the hardware
		Executes, for the most part on the same code
		If it runs at the level of the untrusted module, let it happen
		If not tell it to go fuck itself, limit hardware speed
			If you're not a coward, fuck it yourself
Privileged instructions are instructions that can run at full speed
Unprivileged is self explanatory, I hope

CPU boots in privileged mode
	It starts by setting up the environment for untrusted code
	Jumps to untrusted code in privileged mode
		If you set up properly, OS is protected
If our program ever runs an illegal instruction, it's like a dude shooting a gun at a school in the air
	Harmless, but highly illegal
The longstanding convention
	Put syscall in eax
	Args in  ebx, ecx, edx, esi, edi, ebx
When you try to run privileged in unprivileged, hardware traps you and lets kernel decide what to do with you
	Idea here is to have a protected transfer of control
For modularization, think of a layered approach
	For multi-level kernel, you need to contact manufacturer for more privileged bits

You are not reading from fd 0
	There are no fd 0, 1, 2
	Read from fd in  your array at location 0
Getopt will not get values past the first
	Opt index will do this
		Is the next string of argv
	Moves what it can't understand to the end
Opt index points to string of rest of strings it cannot understand after all processes it can take are understood

order after getopt:
--command 3 
--command 0 
--command 1 
--close 2 
--close 4 
--wait 
5 6 tr A-Z a-z 2 6 sort 1 4 6 cat b-

command 0 -> WRONG if read(0), must do read(array[0])
---
-rdonly 
--verbose - easy
    - does not matter to print, not going to be tested (verbose itself dnm, care abt order)

Call execv in multithreaded, not fork
Calling fork, your child process has a COPY of your fd tables, but both points to the same fd 
File descriptors remain open across execve
	fd maintained by kernel
tr, sort, cat, use exec with tr ..., pass ARGV 
    - ARGV will be { "tr", "A-Z", "a-z", NULL }, if not segfault
    argv should always terminate with NULL PTR

use execvp (char *file, char * const argv[]);
    - file to run
    - argv from main function
        - read exec and execvp
            - remove all code in current process (replace stack etc.)
            - replaced with new program specified in file 
            - signal handler changed to default (cannot use previous signal handler)
            - previously signal handler code is in stack, entire code section is gone
            - cannot access it anymore after execvp, so changed to default 
            - if we want to renew signal, disposition wont be changed,
                - dispositions will not changed to default 
                - segfault 
                    1. catch with signal handler 
                        - changed to default 
                    2. ignore 
                        - keep ignoring
                    3. use default disposition 
                        - stays default
char * argv[]
char * argv_copy[]
- only need shallow copy, not deep copy, so can use for loop to copy 

cannot use stdlib, so cannot use map 
use naive solution 
- create data structure to store pid -> strings

struct{
    pid_t child; //child pid
    int startIndex; //for argv_copy 
    int endIndex;
}

optind = 3, opt arg = 5, continue checking until we see opindex pontin to --command 
if only 2 numbers, should report error

--command 0 1 3, 3 is not valid, should print error msg

dont creat file, only OPEN/WRITE
when writing, cannot use O_CREAT, if WRONLY, assume file exists 

test case: writing to file that does not exist, must print err but continue runnign 
other cmds

read only or write only, still increment numbers,
    - cannot predict if open succeeds or not 

pipe()
	man this shit

use fd1 to write to pipe, use fd0 to write from pipe 

ls | wc 
- shell will run fork and exec 
- close all fd 
- use pipe
- implement smth like this

Not pipe7
	But how to use is there

1 User Commands
    2 System Calls
    3 C Library Functions
    4 Devices and Special Files
    5 File Formats and Conventions
    6 Games et. al.
    7 Miscellanea
    8 System Administration tools and Daemons

---
AD Chapter 3-6
Process is a running program 
Program itself is lifeless, sitting there and not doing anything
OS takes bytes of the program and gets them running
One often wants to run more than one program at once
	This leads to the problem on how to provide more than one CPU
The OS creates the illusion by virtualizing the CPU
	This is created by running one process, stopping it, running another, and so on and so forth
		Basic technique is known as time sharing of the CPU
Time sharing - allows users to run many concurrent programs
	Problem will be performance
	One resource is shared for short periods of times across different entities
Space sharing
	One resource is divided in space so that many entities can access
	It is like partitioning the system
Mechanisms - low level methods or protocols that implement a needed piece of functionality
Context switch - gives OS ability to stop running one program and start running on another given CPU
Policies - algo for making decisions in OS
	Scheduling policy makes decisions using historical info, workload knowledge, performance metrics
		Greedy algo? <- Look into this if you want more
Process - abstraction provided by OS of a  runnign program 
Machine state - whhat a program can read or update while running
Address space - Memory that process can  address
Program counter (IP, Instruction pointer)
Stack pointer/Frame pointer -> Manages the stack for function parameter
What APIs can do
	Create new processes
	Destroy processes forcefully
	Waiting so processes are forced to wait
	Misc. Any control that works in kernel
	Status updates about processes, etc.
Creating process <- no, storks don't deliver this, but wouldn't that be cool?
	Load the code and static data (initialized variables) into memory, into the address space of the program
	Programs usually reside on a disk
	Loading is done early before running the program in earlier systems (EAGER)
	Loading now is done as the program needs them during execution (LAZILY) <-Why Milennials and under are discriminated against
		Done by paging and swapping
	OS needs to allocate memory for the stack
		Used in C for local variables, function parameters, and return addresses
	OS may also need to allocate memory  for the heap
		Normally for the dynamically allocated data
			Normally allocated by malloc()
			Remember to free() Your prisoners, you sick bastard
			Also manages interesting data structures
	OS  Will do initialization tasks regarding file IO
		In linux, the three open file descriptors by default
	OS then and  only then starts to do program execution  and startus by jumping to the main mechanism
		Here control is transferred to the program
Process states
	Running - process is already executing instructions
	Ready - process is ready to run, but OS has not chosen to allow it to run, that fucker
	Blocked - A process has performed some kind of op that makes it not ready to run until other events take place
Schedulers schedule when programs can run
	Process list keeps track of all of this
Register context - holds contents of registers for a stopped process so it can keep it when process resumes
Sometimes, system will hhave initial state in process 
Process could be placed in final if it has exited, but not cleaned up
Process control block - individual structire that stores info about a process
The process is the major OS abstraction of a running program. At any point in time, the process can be described by its state: the contents of memory in its address space, the contents of CPU registers (including the program counter and stack pointer, among others), and information about I/O (such as open files which can be read or written)
The process API consists of calls programs can make related to processes. Typically, this includes creation, destruction, and other useful calls.
Processes exist in one of many different process states, including running, ready to run, and blocked. Different events (e.g., getting scheduled or descheduled, or waiting for an I/O to complete) transition a process from one of these states to the other.
A process list contains information about all processes in the system. Each entry is found in what is sometimes called a process control block (PCB), which is really just a structure that contains information about a specific process
Creating processes can be done by fork() and exec()
	wait() can be used by a process wishing to wait for another process just created
fork() creates a new process
	Strange routine, so be careful withh thhis
Process identifier is known as the PID, returned by getpid()
fork() - how to create a new process
	It creates an almost exact copy of the calling process
	To the OS, it seems like there are twins running around, annoying little fucks
	the  newly created process is thhe child, bc it inherits personalities from the parents
	It comes to life just as if it had called fork, so that would be child PID's main()
		Reasoning is bc infinite threads dummy
	Child has its own copy of address space, but returns differently
Output of fork() is not deterministic
	When a child process is created, we have to worry about both the parent and the child process
The CPU scheduler determines when each process should run
Non-detrminism causes problems in multithreading
wait() causes delay until the child finishes executing
exec() is for when we want to run a different program than the one already running
	this loads code from the executable and overwrites its current static program
	This passes in any arguments as argv and transforms current running program 
Prompt-interface waiting for user interaction to control
Redirection is when shell closes standard output just as the child is being created but before calling exec and opens the newfile
All output is then being set into the file
Sys calls send signals to process including directives 
SIGINT -> interrupt signal ctrl-c that stops and terminates the process
SIGTSTP -> pauses a process istead
signal() catches various signals in ordr to ensure a particular signal is delivered properly
Each process has a name; in most systems, that name is a number known as a process ID (PID)
The fork() system call is used in UNIX systems to create a new process. The creator is called the parent; the newly created process is called the child
	As sometimes occurs in real life [J16], the child process is a nearly identical copy of the parent
The wait() system call allows a parent to wait for its child to complete execution
The exec() family of system calls allows a child to break free from its similarity to its parent and execute an entirely new program 
A UNIX shell commonly uses fork(), wait(), and exec() to launch user commands; the separation of fork and exec enables features like input/output redirection, pipes, and other cool features, all without changing anything about the programs being run
Process control is available in the form of signals, which can cause jobs to stop, continue, or even terminate
Which processes can be controlled by a particular person is encapsulated in the notion of a user; the operating system allows multiple users onto the system, and ensures users can only control their own processes
A superuser can control all processes (and indeed do many other things); this role should be assumed infrequently and with caution for security reasons
Time sharing  run one process for a bit and switch
Performance in virtualization
	OS must virtualize the CPU in an efficient manner while retaining control over the system
	Limited direct execution - running the program directly on the CPU
		When OS wants to start a program running, it creates  process entry, allocates memory, loads program int memory, locates its entry point and jumps to it
	Restricting operations - separating privileged and unprivileged operations
System calls are requests so kernel can check who is looking to use the call
A program must execute a trap instruction
	This raises privilege to kernel mode and allow it
	This calls a return-from-trap when done
	Kernel controls what code to execute upon a trap
	System call numbers are assigned to each system call
Level of indirection serves as a form of protection
LDE protocol
	Kernel initializes the trap table and CPU remembers its location for subsequent use
The OS cannot take action if it is not running on the CPU
Cooperative approach - OS trusts program to yield flow of control
	OS gets control when illegal is tripped
Non-cooperative approach - timer interrupt
	Calls interrupt handler to interrupt the program
Context switch - Makes sure that execution of another process is done rather than the return-from-trap process that was just executed
	And sets registers to point properly
Scheduler - Plans and schedules which programs are set to run when
Disable interrupts - when an interrupt is being handled, no other should be sent
Locking schemes protect clashing signals
The CPU should support at least two modes of execution: a restricted user mode and a privileged (non-restricted) kernel mode. • Typical user applications run in user mode, and use a system call to trap into the kernel to request operating system services
The trap instruction saves register state carefully, changes the hardware status to kernel mode, and jumps into the OS to a pre-specified destination: the trap table
When the OS finishes servicing a system call, it returns to the user program via another special return-from-trap instruction, which reduces privilege and returns control to the instruction after the trap that jumped into the OS
The trap tables must be set up by the OS at boot time, and make sure that they cannot be readily modified by user programs
	All of this is part of the limited direct execution protocol which runs programs efficiently but without loss of OS control
Once a program is running, the OS must use hardware mechanisms to ensure the user program does not run forever, namely the timer interrupt
	This approach is a non-cooperative approach to CPU scheduling
Sometimes the OS, during a timer interrupt or system call, might wish to switch from running the current process to a different one, a low-level technique known as a context switch

SK 4-4.1.2, 5-5.1, 5.3, 5.5
Enforced modularity
	Messages are only way to request modules provide service
		Limits interaction = less risk
	Messages only way for error to propogate
		You can limit and set checkpoints
	Messages are te only way for attackers to attack
		Think of your system like TSA
Client/Service Organization
	Dividing it up into named procedures that call one another
	Limits implementation errors to caller and callee
Soft modularity is caller callee trusting one another
Stack discipline: each invocation must leave the stack as it found it
Procedure calling convention results in a new stack frame
Seg faults in the callee can cause the caller to fail
The procedure call contract provides soft modularity
	If implementation or programmer makes an error, shit can get bad
The client is theh module that initiates a request
	Service is the module that responds by first extracting arguments, performing requested and builds response
	Client then extracts from the response message
A single instance of a service running on a single computer is called a server
Marshaling - converting arguments into canonical representations so the service can interpret
Client/service organization can propogate errors only one way
	Transactions cannot be too complex anyways so less headache
	Client can protect itself bc of timeouts
However wrong responses can cause errors as well
Client/server organization using virtualization
	Seperate first by multiplexing the physical instance or aggregate multiple instances into one physical or emulate a virtual object from one phhysical instance
	RAID is an example of virtualization using aggregation
Processor is virtualized by threads
	Threads consists of a program counter
		References to the environment
Module with one thread has one processor
Humans are better at understanding one process than several concurrent ones
Virtual memory sets up walls so memory addresses can't access one another
With threads and virtual memory, we can create seperated virtual computers that don't know each other exist
Client and service modules on virtual computers use buffers to communicate
	If full, a wait is issued to all
We enforce modularity with domains
	Thhreads can only refer to memory within its own domain
	Register provides an intepreter known as mem manager to enforce this rule
		Placed between processor and bus
	Register knows bounds of domains
		Threads can have more thhan one domain, where they can control that part
Memory manager knows and can give away mem addresses not in use
If two or more threads map a domain, that domain is shared
User mode vs Kernel mode
	User mode gives unprivileged control
	This is normally controlled by a single bit as a signature to tell processor whether or not kernel mode is allowed
Supervisor call instructions allow a gate manager to call appropriate procedures
	SVC must be executed without interruption or thread will have kernel mode
Modularity of the buffer
	Shared buffers are put in a kernel domain so gate registers control passage of instructions
Kernel is the only intermediary between hardware and an application
Monolithic vs Micro Kernels
	microkernel organization is that errors are contained within a module, simplifying debugging
		Each module has its own kernel
	Monolithic kernels are more commonly used
		Each thread typically has an identifier (id), a stack pointer (sp), a program counter (pc), and a page-map address register (pmar), pointing to the page map that defines the thread’s address space
		Exception handler can invoke procedures on the thread's behalf
		exit_thread ( ): destroy and clean up the calling thread. When a thread is done with its job, it invokes exit_thread to release its state.
		destroy_thread (id): destroy the thread identified by id. In some cases, one thread may need to terminate another thread. For example, a user may have started a thread that turns out to have a programming error such as an endless loop, and thus the user wants to terminate it. For these cases, we might want to provide a procedure to destroy a thread.
		Yield: 1. Save this thread’s state so that it can resume later. 2. Schedule another thread to run on this processor. 3. Dispatch this processor to that thread.
		allocate_thread:
			1. Allocate space in memory for a new stack.
			2. Place on the new stack an empty frame containing just a return address and
			initialize that return address with the address of exit_thread
			3. Place on the stack a second empty frame containing just a return address and initialize this return address with the address of starting_procedure.
			4. Find an entry in the thread table that is free and initialize that entry for the new thread in the thread table by storing the top of the new stack.
			5. Set the state of newly created thread to runnable.
		preemptive scheduling is when processor forces threads to share accordingly
			Set timer and wait till the cookies are baked to interrupt your kids' playtime
		Look at pg 272 for thread layering

Inter-Process Communication
Process interactins
	Coordinations with other processes
		Synchronization
		Exchange of signals
		Control ops
	Exchange of data between processes
		Uni-directional data processing pipes
		Bi-directional interactions
Simple Unidirectional Byte streams
	Pipe is created by parent and inhherited by child, who reads std input and writes std out
	Program accepst byte stream and byte stream out based on function
	Each program oper indep, and cannot interact
	Unstructured except implementations by parsers in apps
	Programs should be able to communicate via messages robustly
	Writing out each program int temp file and use that file as inp to next program
	IPC vs File
		If the reader exhausts all of the data in the pipe, but there is still an open write file descriptor, the reader does not get an End of File(EOF)
			Rather the reader is blocked until more data becomes available, or the write side is closed.
		The available buffering capacity of the pipe may be limited
			If the writer gets too far ahead of the reader, the operating system may block the writer until the reader catches up. This is called flow control.
		Writing to a pipe that no longer has an open read file descriptor is illegal, and the writer will be sent an exception signal (SIGPIPE)
		When the read and write file descriptors are both closed, the file is automatically deleted
Named Pipes
	Readers and writerss cannot authheticate
	Writes from multiple writers interspersed, with no indications of which bytes came from whom
	They do not enable clean fail-overs from a failed reader to its successor
	All readers and writers must be running on the same node
Mailboxes
	Data is not a byte-stream
		Rather each write is stored and delivered as a distinct message.
	Each write is accompanied by authenticated identification information about its sender
	Unprocessed messages remain in the mailbox after the death of a reader and can be retrieved by the next reader
Complexity of networks
	Ensuring interoperability with software running under differerent operating systems on computers with different instruction set architectures
	Dealing with the security issues associated with exchanging data and services with unknown systems over public networks
	Discovering the addresses of (a constantly changing set of) servers
	Detecting and recovering from (relatively common) connection and node failures
Protocol stacks may be many layers deep, and data may be processed and copied many times 
Network communication may have limited throughput, and high latencies
Shared Memory
	The network drivers, MPEG decoders, and video rendering in a set top box are guaranteed to be local. Making these operations more efficient can greatly reduce the required processing power, resulting in a smaller form-factor, reduced heat dissipation, and a lower product cost
	The network drivers, protocol interpreters, write-back cache, RAID implementation, and back-end drivers in a storage array are guaranteed to be local. Significantly reducing the execution time per write operation is the difference between an industry leader and road-kill
	High performance for Inter-Process Communication means generally means:
		efficiency ... low cost (instructions, nano-seconds, watts) per byte transferred.
		throughput ... maximum number of bytes (or messages) per second that can be transferred.
		latency ... minimum delay between sender write and receiver read.
	The fastest and most efficient way to move data between processes is through shared memory:
		create a file for communication.
		each process maps that file into its virtual address space.
		the shared segment might be locked-down, so that it is never paged out.
		the communicating processes agree on a set of data structures (e.g. polled lock-free circular buffers) in the shared segment.
		anything written into the shared memory segment will be immediately visible to all of the processes that have it mapped in to their address spaces.
	Moving data in this way is extremely efficient and blindingly fast ... but (like all good things) this performance comes at a price:
		This can only be used between processes on the same memory bus.
		A bug in one of the processes can easily destroy the communications data structures
		There is no authentication (beyond access control on the shared file) of which data came from which process

---

OS Organization
	Interface stability
int fd = open("foo.txt", O_RDONLY...)
	X remote_open("lnxsrv07", "foo.txt", O_RDONLY); API extend if for remote files
	X open("lnxsrv07:foo.txt", O_RDONLY);
	X open("foo.txt", O_RDONLY | O_REMOTE, "lnxsrv07");
	open("//lnxsrv07/foo.txt", O_RDONLY);
	filesystem + mounting (later)
	open("/home/eggert/foo.txt", O_RDONLY);

OS Organization via virtualization
	What resources are available? (that the kernel protects)
		Faster
			ALUs
			Registers
			Cache
			Primary memory (DRAM)
		Slower404
			I/O Devices (Flashes, displays, network, 
		We have to control the time that  is being used
			Avoid time hogs
				F infinite loops
Virtualizeable processor = hardware support for building virtual machines efficiently
	Each process is a program running atop vm
		Each process has its own registers and ALUs
		Does everything at full speed bc of this
	Each process has primary memory
		Needs all of this for stackframes and shit
		All of this is basically RAM
		Harder than the first part
	Cache will be shared, as it is a magical assumption in the cache take care of itself
		It's like the assumption that I take care of myself
		People attack based off of violation of this
	Access to I/O is prohibited to process except via a syscall

new process(); X
pid_t fork(void);
	Creates a clone of a process
	The old is called the parent
	New is called the child
	Returns 0 in child
	-1 if fail XOR pid in parent
Fork continually
	while(fork())
		continue;
	exit(27);
FORK BOMB // BC FUCK YOU THAT'S WHY
while(true)
	fork();

pid_t fork(void);
int execvp(char const * file, char * const * argv); [same vm different program]
	argv is what gets passed to main in the executable named by "file"
	Does not wait for program to finish
	Destroys the calling  process
	Returns int for fail safety and sets errno
pid_t waitpid(pid_t pid, int *status, int flags);
_Noreturn void_exit(int status);
	0-255 is passed
	sometime 0-127
	look at system descriptions
	0 is normally success
	OS doesn't really care, like Eggert regarding our sleep

pid_t p = fork();
if(p < 0) {
	error("...");	
}
if(p == 0) {
	execvp(...);
	perror("failed");
	exit(27);
}

#include<sys/wait.h>
bool print_time_of_day(void) {
	pid_t p = fork();
	int status;
	if(p == -1)
		return false;
	else if(p == 0) { // this is child, any sexual activity done to it is rape
		execvp("usr/bin/date", (char[]){"date", 0});
		_exit(126);
	}
	else {
		if(waitpid(p, status, 0) < 0)
			abort(); // dumpcore and crash
		return WIFEXITED(status) && WEXITSTATUS(status) == 0;
	}
}

fork(); // clone process
	1. Return value (%rax)
	2. Pid, ppid
	3. Accumulated execution times
	4. Child locks (child has none)
	5. Pending signals
execlp, execvp is opposite
	All preserved
	Others are replaced

int kill(pid_t, int); //send signal INT to process p
kill(p, SIGINT) = works only if sender and receiver have same pid
kill(getpid, SIGINT) = exit(126)

Zombie process - dead process we cannot recycle
	Happens when child exits and parent doesn’t know it
	Use waitpid to fix

alarm(100);-> sends SIGALRM 100 seconds in
	Alarms survive exec
	Child will have a death sentence essentially
	This, however, can be screwed with

Sometimes if waitpid not called, we can accidentally fork bomb and create tons of zombies
	Parent exits and leaves child, which will clog up the system
	Special case to fix is to have OS reparent the child processes

init.c kills the orphans and destroys zombies via
	while(waitpid(-1, &status, WNOHANG) > 0)
		continue;

Next version of newmake  is
	int posixspawnvp(pid_t * restrict pid,
	char const * restrict file,
	posix_spawn_file_actions_t const * file_acts,
	posix_spawn_attr_t const * restrict attrp,
	char * const * restrict argv,
	char * const * envp);
		Does the fork and exec, does it on the files and tells you whether or not it works
		Restrict says to implement faster but don’t do anything tricky
		Faster for big programs but complicated as fuck

Processes run in isolation
How much isolation
	Total isolation? NO
	Counterexamples (exceptions)
		fork() - conveys information about existence
		waitpid -convey status, "time" to parent
		kill(sig, pid) - comm signal #, "time" to any process
	Isolation
		Ease of debugging
		Security
Big data communication but still keep isolation's good stuff	
	Files
		a < f - cannot run in parallel
		a > f - cannot run in parallel
		a | b - can run in parallel
	Message-passing
	Shared memory
	Signals, waitpid, ...
	Covert channels
		Complete isolation 1 CPU
I/O access to files
	Slow compared to CPU/a bit of CPU overhead is fine
	Robustness/security is an issue
	Lots of kinds of devices - tempting to have complex API deal with them
		Avoid thhis, APIs should be simple and portable to any device
Two major kinds of devices 
	Storage
		Flash, disk, tape
		Request/Response
		Random access
		Finite size
	Stream 
		Display, keyboard, network adapter
		Spontaneous data generation
		Most recent data is received
		Potentially infinite

Unix big idea: treat everything as a file
	int open(char const * file, int flags, ...);
	int close(int fd); 
	ssize_t read(int fd, char * buf, size_t bufsize);
	ssize_t write(int fd, char const * buf, size_t bufsize);
Problems with the above approaches for networking
	What is the file size^?
		Networking just streams
		Infinite data
	OS buffers/event handling?
		How to make sure that data read doesn't stop process and process can handle quickly
	Random access for storage devices
		We can't just access any data we want
		We can only access data as it streams
Can help overcome random access if set in flag of off_t /seek(int fd, off_t offset, int flag)
	The command returns 1 if error and sets errno
	SEEK_START
		interpret file wrt start of device
	SEEK_END
		interpret file wrt end of device
	SEEK_CUR
		interpret file wrt current packet from device
Orthogonality
	Independent features
		Simple, complete, combinable
int fd = creat("./foo.ch", 0666);
open("foo", O_RDONLY);
open("foo", O_RDONLY); // Originally
open("foobar", O_RDWR | O_CREAT, 0666); // After linux patch
	0666
		-rw-rw-rw
lseek + read = pread OR pwrite = lseek + write
	Common in Random access
	lseek is to  have something for storage devices but leave it alone
		Orignally good, now not

Deal with filenames
	Orthogonality fights with performance
		Setting up walls everytime kills performance
		Like using malloc and realloc instead of just giving a massive ass buffer
	char const unlink("file");
		Removes a file
		Returns -1 if it couldn't
	char const unlink("file1", "file2");
		Renames a file
		Returns -1 if fail
	link 
	...

Process 1
	fd = open("f", O_RDONLY);
	read(fd, ...);
	read(fd, ...);
Process 2
	...
	unlink("f");

Process 2 should not mess with process 1...
	As long as file was opened BEFORE removal

$ touchh secret
$ ls -l secret
-rw-r--r-- 0 ______ secret
------> attacker uses (sleep 100; cat) < secret
$ chmod 600 secret
$ ls -l secret
--rw------ 0 ______ ______
$ echo 'password!?' > secret
--- cat runs

This is what is known as a race condition

int fd = create_temp // Returns -1 and sets errno on fail

int create_temp(void) { // race condition is if it creates and reads at the same time
	int fd = open("/tmp/foo", O_RDWR | O_CREAT, 0600);
	if(0 <= fd)
		if(unlink("/tmp/foo") != 0) 
			abort();
	return fd;
}

int create_temp(void) { // fix condition
	int fd = open("/tmp/foo", O_RDWR);
	if(fd <= 0) {
		close(fd);
		errno = EBUSY;
		return -1;
	} // someone can run a different execution as we run this, i.e. race still happens due to if-then
	fd() = open("/tmp/foo", O_RDWR | O_CREAT, 0600);
	(cat a& cat b) > file // shell runs cat a in the background and runs b in parallel a& indicates running in bg
	// will output to a single filestream
	// output will be interweaved
	// can also do (cat > a& cat > b)
	// that will split file into files a and b
	do {
		char buf[100];
		sprintf(buf, "/tmp/foo%d", random());
		int fd = open(buf, O_RDWR | O_CREATE, 0666);
	} while(fd < 0  && errno == EEXIST); // Problem is if we want a big file, we fill up the file system -> we will lock up ourselves
	// ls -l will find nothing bc they write their own code -> but they can find you in the end
}

O_EXCL
	If file already exists, fail
	You must create the file

What can go wrong with fds
	Race conditions
	Open but unnamed files
	File descriptor leaks
		open(...) opens too much and returns EMFILE -> TOO many files open so system cannot probe all files
		Happens due to not closing a file
	access fds not open
		EBADF -> FILE not opened
	I/O Error
		You have a bad drive, which returns -1 and raises EIO error
			SPOOKY SCARY SHIT
	EOF 
		Error will read, but there is no actual data in it aside from EOF
			This is fine
	Stream, but no data in it
		Read(...) => Both options are wrong
			Wait
			Returns -1 and sets ERRNO
				EAGAIN
	Device no longer there
		Flash removed, etc
			Returns -1 and sets errno

Return with highest exit status including child processes

---

AD Chapter 7, 8, 26-27
Workload - processes running in the system
Assumptions about jobs running
	Each job runs for the same amount of  time
	All jobs arrive at the same time
	Once started each job runs to completion
	All jobs only use the CPU
	Run-time is known
Turnaround time
	= time completion - time arrival
	Performance metric
Fairness
	Performance and fairness are often at odds in scheduling
FIFO
	Example of simple algo for scheduling
	Convoy effect
		A number of short potentials get queued due to heavyweight
Shortest Job first
	Think greedy algorithm, but based on time bc we don't have a certain timespan
	Optimal if all jobs come in together
Shortest time to completion first
	Greedy algorithm
	Relax assumption 3
		Use context switching 
Response time
	Time of first run - time of arrival
Round Robin scheduling
	Runs a job for a time slice (schheduling quantum)
	Switches to next job in run queue
	Continues till finish
	Interrupt must be a multiple of timer-interrupt period
Any policy that is fair is going to work poorly on metrics suchas turnaround time
	This is the trade-off
Scheduler must make scheduling for I/O bc it blocks that process until I/O is done
	Here, an interrupt is raised and OS uns and moves process issued the I/O from blocked to ready state
The scheduler above ^ is known as multi-level feedback queue
Multi-level feedback queue (MLFQ)
	Optimize turnaround time
	Minimize responce time
MLFQ has a number of distinct queues that decide which job to run at a given time
	A job with high priority is chosen to run
	If jobs have equal, run Round robin protocol
	When job enters the system, it is placed as highest priority
	If job  uses up entire time slice while running, it moves down one queue
	If job gives up before time slice, it stays at the same level 
	We let I/O intensive jobs run quickly
The Current MLFQ Problems:
	Starvation: There are too many interactive jobs running and combines to consume all CPU time
	Game the scheduler
		You trick the scheduler into giving you more than the fair share of the resource
	Changing behavior
		Difficult to control and predict
More rules: Priority Boost
	After some time period S, move all jobs in system to topmost queue
Better accounting
	Once a job uses up time allotment at  a given level, its priority is reduced
Advice sometimes can be used to set priorities
Threads allow for parallelism
	Sometimes lead to race conditions which output nondeterministic results
Critical section - piece of code that accesses shared variable and should not be run together
	Must use mutex

SK Chapter 5.2, 6.3-6.3.3, 9.1.2-9.1.7
Pipes allow two programs to communicate using file system call interface
With a bounded buffer, several threads running in parallel and on seperate physical processors can accidentally edit the same buffer
OS may provide following interface: 
	buffer <- allocate_bounded_buffer (n): allocate a bounded buffer that can hold n messages
	deallocate_bounded_buffer (buffer): free the bounded buffer buffer.
	send (buffer, message): if there is room in the bounded buffer buffer, insert message in the buffer. If not, stop the calling thread and wait until there is room.
	message <- receive (buffer): if there is a message in the bounded buffer buffer, return the message to the calling thread. If there is no message in the bounded buffer, stop the calling thread and wait until another thread sends a message to buffer buffer
Thread coordination
	Programmer = god (Genius approach)
	Programmer = shit (Database approach)
Problem with sharing a buffer is the producer and consumer problem
	Consumer and producer must coordinate
	Sequence coordination
		Coordination in which both threads must wait for one another
One-writer principle
	If each variable has only one writer, then coordination becomes easier
Race conditions - Allowing multiple senders and receivers, where you don't know which one comes in first
We cannot be sure how memory ops executes in two threads, only that they are serial
Locks and before-or-after actions
	Lock is a shared variable that acts as a flag to coordinate usage of other shared varriables
	A thread may ACQUIRE a lock, hold it for a while, and then RELEASE it
	buffer
	The lock flags usage of a variable for one and only one thread until it is done
	Locks do not prevent it completely but acts as a flag error when other threads try to access it
	A lock can be used to implement before-or-after atomicity
		During the time that a thread holds a lock that protects one or more shared variables, it can perform a multistep operation on these shared variables
	The database literature uses the terms isolation and isolated actions; the operating system literature uses the terms mutual exclusion and critical sections; and the computer architecture literature uses the terms atomicity and atomic actions
	Single-acquire protocol - one thread can acquire a given lock at a time
Deadlock
	Threads have to continually wait for each other locks and refuse to give up contol until the other one gives up
Implementing locks must enforce single-acquire locks
All-or-nothing atomicity
	A sequence of steps is an all-or-nothing action if, from the point of view of its invoker, the sequence always either
		completes,
		or
		aborts in such a way that it appears that the sequence had never been undertaken in the first place. That is, it backs out. 
Before-or-after atomicity 
	Concurrent actions have the before-or-after property if their effect from the point of view of their invokers is the same as if the actions occurred either completely before or completely after one another.
Atomicity 
	An action is atomic if there is no way for a higher layer to discover the internal structure of its implementation.

User-mode threads
Processes are expensive to create and dispatchh
	Each has its own virtual address space and ownership of resources
Each process operates privately and cannot share with parallel processes
Threads
	Independently schedulable unit of execution
	Runs within address space of a process
	Accessable to all system resources owned by that process
	Has private general registers and private area of the stack frame 
Process is a container for address and resources whereas thread is the unit of sceduled execution
Thread model
	Each time a new thread is created
		Allocate memory of a fix-sized for thread-private stack from the heap
		Create thread descriptor that contains identical info, scheduling info, and pointer to the stack
		Add  a new thread to a ready queue
	When thread calls yield() or sleep(), save general registers (including stack pointer) and select next thread from ready queue
	To dispatch a new thread, store its saved registers and return call that caused it to yield
	If thread calls sleep()
		Remove it from ready queue 
		When ready
			Put back on ready queue
	When thread exits
		Free stack and thread descriptor
Linux processes can schedule SIGALARM timer signals and register a handler for them
Before dispatching, we can schedule SIGALARM if thread runs too long
If thread runs too long, SIGALARM yields the thread
Problems impelmenting using user mode
	When system call blocks
		Threads all block
		OS doesn't know of other available threads
	Exploiting multi-processors
		OS can schedule processes to run in parallel on this
		Threads cannot execute in parallel if OS is unaware
Both problems can be solved if threads are implemented by OS
If non-preemptive scheduling can be used, user-mode threads operating in with a sleep/yield model are much more efficient than doing context switches through the operating system. There are, today, light weight thread implementations to reap these benefits.
If preemptive scheduling is to be used, the costs of setting alarms and servicing the signals may well be greater than the cost of simply allowing the operating system to do the scheduling.
If the threads can run in parallel on a multi-processor, the added throughput resulting from true parallel execution may be far greater than the efficiency losses associated with more expensive context switches through the operating system. Also, the operating system knows which threads are part of the same process, and may be able to schedule them to maximize cache-line sharing.
Like preemptive scheduling, the signal disabling and reenabling for a user-mode mutex or condition variable implementation may be more expensive than simply using the kernel-mode implementations. But it may be possible to get the best of both worlds with a user-mode implementation that uses an atomic instruction to attempt to get a lock, but calls the operating system if that allocation fails (somewhat like the futex(7) approach).

Real-time scheduling
Priority based scheduling is a best effort approach to give better service to certain processes
	Everything has a time limit to execute
A real-time system is one whose correctness depends on timing as well as functionality
Traditional scheduling looks at turn-around time (throughput), fairness, and mean response time
	Real time system are characterized by
		Timeliness - meeting timing requirements
		Predictability - how much deviation is there in delivered timeliness
		Feasibility - if it is possible to meet requiements
		Hard real-time - hard deadline meaning it has to be done in x time
		Soft real-time - we can miss response time, but consequence is only degraded performance/recoverable failures
Real-time scheduling is moe critical and difficult than traditional time-sharing but:
	We know how long each task will take to run
		Enables intelligent scheduling
	Starvation may be acceptable, as we don't need every task all the time
	Work-load may be relatively fixed
		Incoming traffic is often constant so not much to do
Sometimes there may not be a scheduler in simple real-time
You may do static scheduling for more complex
	Build a fixed schedule based on tasks
Dynamic scheduling is normaly run
	Choose next task via greedy algorithm
	Handle overload
		Best effort
		Periodicity adjustments ... run lower priority tasks less often
		Work shedding ... stop running lower priority tasks entirely
Preemtion may be used in a different issue in real-time systems
	Means of improving means by dividing and conquering
	Prevents infinite loops
	Trade off is increased overhead
		Real-time systems can
			Miss deadlines due to this
			We have little need for preemtion bc of fixed time rates
			We normally run less complex stuff
Linux now supports a real-time scheduler, which can be enabled with sched_setscheduler(2)
Windows is not suited for real-time applications
---

OS Organization Control
	File descripteros and pipes
Signals 
Threads, Scheduling
$cat file > output
int fd = open("file", O_RDONLY);
	returns 
read(fd, -)
write(1, ...)

Shell
	pid_t p = fork();
	if(p == 0) {
		int fd = open("output", O_WRONLY | O_CREAT, 0666);
		dup2(fd, 1);
		execlp("/usr/bin/cat", (char *[]) {"cat", "file", 0});
		error(); // error coding
		exit(127);
	}

Pipes:
	Control/make safe IPC
	Pipes can be accessed like other stream files
	int pipe(int[2])
		int[2] denotes address of two ints 

A pipe is a bounded buffer
the read pointer
	chases writer pointer
	Copies data it passes over 
write pointer
	wraps over past end of buffer
Circular buffers
	Think signed bit overflow
write(8, buf, 4096)
	Will wait until room in pipe

if du | sed -n; then
	echo OK
else
	echo OUCH
fi

pid_t b  = fork();
if(b < 0) error();
else if(b  == 0) {
	int pipefd[2];
	if(pipe(pipefd) < 0)
		error();
	pid_t a = fork();
	if(a < 0)
		error()
	if(a == 0) {
		dup2(pipefd[1], 1);
		close(pipefd[0]);
		close(pipefd[1]);
		execvp("du", (char * []) {"du", 0});
	}
	else {
		dup2(pipefd[0]);
		close(pipefd[1]);
		execlp("/bin/sort", (char *[]) {"sort", "-n", 0});
	}
	if(waitpid(b, &status, 0) < 0)
		error();
}

pipe failures
	tridy @ C level
	Read, but no writer => 0 return (EOF)
	Write, but no reader
		(default) - SIGPIPE signal
		(option) return -1, errno = ESPIPE

int p[2];
pipe(p);
for(i = 0; i < n; i++)
	write(p[1], buf, 1000);

SIGNALS
	Workstation with Emacs <-> UPS/Battery -> House
	What to do when power goes out
		Do nothing and lose work
		OS Saves state of all running processes etc into flash
			Problem is when process is frozen
			It can go haywire
		End-to-end approach: processes get notified
			Processes are interrupted and silenced so OS can announce -> similar to eggert's class
	How to notify
		/dev/power -  continues "1" if okay, "0" if about to power down
			Processes poll /dev/power
				Everyone hates this method bc more  work
		/dev/poweringoff
			Processes block reading from /dev/poweroff
			A problem on how to NOTIFY
				Solvable but eh
		Signals
			Lets processes grab one another and tell it info needed

Reasons for signal #include <signal.h> -> #define SIGINT 2
	Power outage - SIGPWR
	Out of control processes - SIGINT, SIGKILL, SIGQUIT
		This is first degree murder
	Invalid programs - SIGILL, SIGFPE, SIGSEGV, SIGBUS
	I/O Errors - SIGIO, SIGPIPE
	A child died - SIGCHLD
		Fucking finally though
	User lost interest - SIGHUD
		Just like my last 3 exes
	Timer expired - SIGALBM
		BOOM
		Oh nvm

Signal delivery
	-ignore (except SIGKILL)
	-terminate (possibly dumping core)
	-call a function that is defined in the code the process is running
		Not default

API to specify  how signals are handled
	sighandler_t signal(int sig, sighandler_t fn);
	typedef void (*sighandler_t)(int);

void handle_ poweroff(int sig) {
	save_file(); // Examines global data striucture
	write("OK boss\n");
}

int main(void) {
	signal(SIGPWR, handle_poweroff);
	.
	.
	.
	// other code
	// Block SIGPWR
	// Change data stuct - SIGPID
	// Unblock SIGPWR
		// This is a critical section
}

int pthread_sigmask(int how, sigset_t const * restrict set, sigset t * restrict oset);

Midterm next week
Discussion about it
UPE Session Tuesday

Time management for CPUs
	Signal HHandling is about this
		big problem-races (soln: block signals)
			e.g. printf in a signal  handler

Async signal safe function:
	main code:
		printf("abc%d", n++);
	signal handler:
		printf("oops");

printf
malloc

Threads: Like processes but share memory
	+ Performace of IPC
	- Races

Process resources: Each process has their own
	Address space
	File descriptor table
	Signal handler table
	Working directory (chdir), root directory(chroot)
	Unmask
		$ umask 0755
		$ cat /etc/passwd > foo
						  ^ open("foo", O_WRONLY | O_CREAT, 0666); // -rw-r--r--
						  0666 & ~0755 -> 0644
	/uid/gid/...
	------------------------------------------- Above is per process (expensive, security related), below is per thread (cheap and efficient)
	Stack
	Registers
	State (Zombie? Running?)
	Signal mask
		We want to be able to establish a critical section PER thread
	errno #include<errno.h>
		Implemented by 
			#define errno(*__errno())
			extern int * errno(void);
The above is why processes become expensive as the amount increase

Remember that threads exist inside processes
Per thread (Transparent)
	pthread_create
	pthread_exit
	pthread_join
	pthread_t
		Can be a pointer
Per process (Opaque)
	fork + exec
	waitpid
	exit
	pid_t

Opaque is also being used to mean hidden, which is perhaps where the confusion comes in. The term opaque type has a specific meaning in C/C++, where it refers to a type that has been declared but not yet defined.
Transparent is being used to mean hidden in the sense of things taking place automatically behind the scenes 
In both cases, I think people are using these terms to express a lack of visibility. Transparent is used where something is present, but you can't see it. Opaque is used where something is present, but you can't see inside it to inspect its inner workings

Scheduling: assigning CPU/Cores/Hardware IPs to threads
Hyperthreading - one mult unit, two IPs
	If one does one mult, then full speed 
	If both, slow down
Theory
	Scheduling Policies
Practice
	Shceduling Mechanism

Context switch
	storing the context or state of a process so that it can be reloaded when required and execution can be resumed from the same point as earlier
	Thread volunteers
		Threads typically volunteer anyways -> System call
		This is mostly in embedded/smaller systems bc hassle for reliable programs
	Thread preempts a thread
		Works even if thread infloops
		Big problems 

Context Switching Triggers
	Multitasking: In a multitasking environment, a process is switched out of the CPU so another process can be run. The state of the old process is saved and the state of the new process is loaded. On a pre-emptive system, processes may be switched out by the scheduler.
	Interrupt Handling: The hardware switches a part of the context when an interrupt occurs. This happens automatically. Only some of the context is changed to minimize the time required to handle the interrupt.
	User and Kernel Mode Switching: A context switch may take place when a transition between the user mode and kernel mode is required in the operating system.

Context Switching Steps
	The steps involved in context switching are as follows:
		Save the context of the process that is currently running on the CPU. Update the process control block and other important fields.
		Move the process control block of the above process into the relevant queue such as the ready queue, I/O queue etc.
		Select a new process for execution.
		Update the process control block of the selected process. This includes updating the process state to running.
		Update the memory management data structures as required.
		Restore the context of the process that was previously running when it is loaded again on the processor. This is done by loading the previous values of the process control block and registers.

#include<sched.h> // int sched_yield(void); <- inb + outb + insl

inb + outb
	while(isbusy(dev))
		continue;
	BUSY WAITING

	while(isbusy(dev))
		sched_yield();
	POLLING

	while(isbusy(dev))
		"wait for dev to be ready"
	BLOCKING
		OS must keep track of which threads are  blocked

States of threads
	Runnable
	Blocked

Source code to linux kernel (Abridged)
	for(;;) { // Linux scheduler mechanism
		choose thread t that isn't blocked;
		load t's state into CPU;
		(t now runs)
		Sched_yield(); 
		store CPUs state into t
		for(each thread that has become ready)
			unblock(p);
	}
	Use timer interrupt (for uncooperative threads)
		Motherboard has 100Hz clock that makes CPU trap
		Trap is when thread is synchronous interrupted by exceptional condition (1/0) -> drops to kernel -> OS chooses different threads to run -> then returns to you later
		More interrupts = more overhead

Overhead
	Any combination of excess or indirect computation time, memory, bandwidth, or other resources that are required to perform a specific task

inflop kill(...)
	^C
	SIGXCPU

Realtime scheduling
	Hard deadlines real-time
		You should not miss a deadline
		Predictablity trumps performance
		Disability caches is common
		Use polling, not interrupts
	Soft scheduling (video player)
		Some deadlines can be missed, if there's too much work
		Earliest deadline first policy
		Rate-monotonic scheduling

Priority scheduling
	SEASnet
		root (superuser)
		ops staff
		faculty
		student

Scheduling metrics
	arrival -> exec -> output -> finish
	Wait time = execution - arrival
	Response = output - arrival
	Turnaround (latency) = Finish - arrival

Throughput/utilization
	What percentage of time is actual work being done?
Fairness
	Do you treat everyone fairly?
Latency
	Third major category

Bitwise operators
& -- The AND operator compares two bits and generates a result of 1 if both bits are 1; otherwise, it returns 0.
| -- The OR operator compares two bits and generates a result of 1 if the bits are complementary; otherwise, it returns 0.
^ -- The EXCLUSIVE-OR operator compares two bits and returns 1 if either of the bits are 1 and it gives 0 if both bits are 0 or 1.
~ -- The COMPLEMENT operator is used to invert all of the bits of the operand.
>> -- The SHIFT RIGHT operator moves the bits to the right, discards the far right bit, and assigns the leftmost bit a value of 0. Each move to the right effectively divides op1 in half.
<< -- The SHIFT LEFT operator moves the bits to the left, discards the far left bit, and assigns the rightmost bit a value of 0. Each move to the left effectively multiplies op1 by 2.

Midterm
-----------
sample midterm on CCLE 
- bring solutions to lab and spec to midterm 

Soft vs hard modularity
- soft: 
    requires different modules to trust each other 
    caller and callee to trust each other 
    eg. shared memory used by different caller and callee 
        caller must trust callee to not override their allocated memory 
- hard:
    client server model 

1. ubuntu uses both hard and soft 
    - if segfault etc, system still keeps running 
    - dont even need to define what hard and soft are 
    - ubuntu has both soft and hard 
        - program that crash does not crash OS (hard)
        - if running kernel level code, can put stuff in register that 
            we are not supposed to, then crash other code, so soft modularity 

2. about lab0 
    - four as stdin, --output=.... --output ago 
    - always process the last one 
    - details like whether it 
    - know pipe, echo, input flag, output flag, 
    -> four to STDOUT, pipe, reads from STDIN (read end of pipe) and writes it 
        to ago file, read the last argument, open the file, and write it 
    -> if writes to ago, and creates all these files, ALSO FINE 

3. can we still use preempting scheduling if we flip it?
    - in linux, any process that runs executes interrupt 
        - interrupt to pass CPU control to kernel 
        - very little overhead and low latency 
    - return of interrupt, should know address of syscall to know where to return 
    - does not really matter because both are valid 
    - validity: it works 
    - performance: using interrupts is better than RETi since we have to keep track of 
        stack addresses 
    - must keep track of return address for interrupt 
    - valid idea:
        - overhead since interrupt has to keep track of return address 

4. 
./simpsh 

1. initialize all files and file descriptors 
    - head 
        - reads from b (STDIN)
        - redirect stderr to a
            - need truncate 
    - sort 
        - append error to c 

know what < and > does, the open flags

./simpsh --creat --trunc --wronly a \ 0
    --rdonly b  1 
    --pipe  2 3 (read, write)
    --creat --append c 4 
    --pipe  5 6
    --creat --rdwr --trunc d 7 
    --pipe 8 9 
    --command 1 3 0 head -n 20 
    --command 2 6 4 sort 
    --command 5 7 (7) tail 
    --command 7 9 9 cat 
    --command 8 7 7 cat 

4b. translation differ:
    - cannot write to shell's STDERR 

4c. loop indefinitely 
    - need cycle 
    - pipe from read to write, circular pipe 
    - 2nd line, take input from d, pipe to second command which outputs it back to d 
    - if d is empty its ok 
    - if d has small no of char, way writes work thers a buffer 
    - if cat's buffer is big enough to store d, writes to second file then done 
    - if file d is way too large, reads enough write to d, cycle 
        -> reading and writing to the same file 
        -> deduce that reading and writing from the same life if looping 

4d. 
    backward compatible: build new features on system while still supporting prev features 
    upward compatible: fine if break things, as long as we have a new behavior 
    - minimal upward-compatible, don't change a lot of things in the interface 

    - start with a = 3, reserve 0 1 2 to be STDIN, STDOUT, STDERR 
        - if we do this, all numbers in --command will change 
    - -1 -2 -3 stdin out err, so don't have to change 
    - subcommands for --command 
        - do --command 
            -i _ -o _ -e _ -cmd _
        - if missing, use STDIN OUT ERR, if not, use specified file descriptor 

4e. 
    --command echo fooled ya , --abort 
    - may put in a sleep statement 
    command sleep, echo, abort but dont use wait flag 
    - how long to sleep? sleep 4-5s

5. round robin 
    R2R has 50 percent chance of not switching

    P1 P2 P3
    P2 P3 P1 
    P3 P2 P1 (RR)

    P1 either move p1 to back with 50% chance, or keep doing it for 1 more quantum(1 time period)

5a. 
    wait time for T2R is going to be worse 
        50% chance to NOT MOVE ON TO SECOND PROCESS 
    utilization is better, 50% chance of not doing context switch 

    average wait time:
        RR: 0 + 1Q + 2Q / 3 = 1Q
        T2R:
            1 + 1/2 +1/4 ...+1/2^n = 2 keep selecting P1 
            increase average wait time by 2x 

5b. starvation?
    yes, may end up selecting same processes everytime 
    think infinity: averag wait time is actually 2Q, not 1Q
    RR is fair, T2R is not, RR - each process gets sameamount of time,
        T2R processes gets time based on luck 

Must write/read code 

6a. char n, can take 0-255 
    - output 256 lines and run program without signal 
    - after printf n from 255 to 0, catch signal, signal goes from 0 + 1
    - assume n = 0

6b. while printing is being printed, n=5, before N++ happends, in between short periods 
    whill say got signal with that n, n=5, so n=N two times 

6c. similar to 6b, before printf finishes, signal runs 
    when printf is printing, printf is not async signal safe, signal runs in async 

    get two signals back to back, sometimes it breaks 
    - READ SIGNAL MAN PAGE 
    - if we want to print using signal handler, set flag, if see flag, exec printf 

6d. dumpcore
    get two signals back to back 

6e. second signal call in while loop
    set signal handler, doing it once above is good. in while loop = redundant 
    return 0 is also not needed, but not the right answer


7. how to read from disk? 
    wait_for_ready:
        see if bus is ready or device is ready 
    read_sector;
        outb, stores 2nd param into 1st param 

7a.
    messing with other register, get garbage value, etc. 
    soft modularity here, need to trust other shit 

7b. 
    does not matter, gets ignored. MSB gets ignored or ?

7c.   
    does not matter, both write to diff parts of memory 

7d.
    havent set all 4 offsets, might be set to something else to prev call to read sector 
    so 1f7, might read a bogus sector which  is bad 
    ORDER MATTERS 

7e. 
    after insl, read from sector, done using it, return 
    does not matter if we wait for ready since we are done usingt he sector and it is 
    a redundant call 
    not going to do anything 

8.
    fork()
    -1  - fail 
    0   - child process 
    PID - parent process 

    if first fork fails,
        call second fork 
        if second fork fails 
            return 
        else 


    build a tree 
    if both pid_t are parent 
        parent num1
            second fork()
                if num2 > num1 (MORE LIKELY)
                    true 
                else 
                    false


simpsh cannot do stderr, so for tail command, since no redirect of stderr, put 7 temp     

    >d and >>d, open as --trunc and close, then open as --append ?
     MSB gets ignored or ?


Lab1C
-----------
getrusage 
- 

know when we start program 

func get_usage(start_time) at beginning of getrusage 

.. profile 


case 'profile':
    get_usage(end time)
    print(end - start)

RUSAGESELF / RUSAGECHILDREN 

test cases, benchmarks, run bash and dash 

in parent, dont close fd , ONLY IN CHILD 
- close write end of pipe in child process, not in parent

getrusage(who, &time) 
function fills the empty struct (2nd param)

rusage myUsage;
if(getrusage(who, myUsage) < 0)
    error ...
else 
{
    myUsage will have information on usage of process
    - usertime and systemtime is what we care about 

    time -> utime_
    s 
}

Scheduling
	Policies
Critical sections
Synchronization
Single-CPU Policies
	e.g. First Come First Serve
		Queue of thhreads waiting to run let each  thread run until done
		Overhead exists between start and end of two processes
		+ Favors long jobs
		- Convoy effect
	Shortest Job First
		Give the shortest job the first benefit of processing
		You have to assume that the runtimes of the jobs are known
		Unfair and starvation is possible
		Rarely used
		This is done wrt jobs coming in time
		+ avg wait/turnaround better than FCFS
		- Unfair: situation call happen
	Last come first serve
		Opposite of FCFS
		Worse than FCFS
	Longest job first
		Worst possible planning time
		CONVOY EFFECT
	Round Robin
		FCFS + Preemtion
		Fair if New jobs put at the end of the queue

Priority Scheduling
	Jobs also have priorities
	Terminology problem - higher priority jobs have lower priority numbers
	ln Unix/Linux etc. process  have "niceness"
	$ ps ... -19 to 19
	$ nice +5 gcc foo.c
	Only root can lower niceness
	When hybrid with RR, it makes it unfai

Preemption is the act of temporarily interrupting a task being carried out by a computer system, without requiring its cooperation, and with the intention of resuming the task at a later time

int balance = 0;

bool deposit(int amt) {
	if(amt < 0 || INT_MAX - amt < balance)
		return false;
	balance += amt;
	return true;
}

bool withdraw(int amt) {
	if(amt < 0 || balance - amt < 0)
		return false;
	balance -= amt;	
	return true;
}

When parallelized, the above functions become a race condition
	This should be a critical section, as we access one resource with two threads, so there should be lock control
	Now assume that you have checking and savings accounts, note that there can be a deadlock

Messed up synchronization
	1. Two CPUs running simultaneously
	2. One CPU + preemtive scheduling
	3. Signal handling

Threads do action sequences
	Thread 1 does its own stuff
		Eventually does actual useful stuff
	Thread 2 does its own stuff
		Eventually does actual useful stuff while thread 1 does stuff
	This means that 1 and 2 can both try to manipulate or edit one specific file and that is big no no bc fuck you
		C does not manipulate and control threads for you bc it expects you to know how to do that

Critical section - sequence of instructions that have to be executed together without interference from any other thread or from a signal handler
	How to find?
	One option is each 0-0 method is a critical section
		Each object has a lock that prevents other threads/handlers from running

bool transfer(int amt, int fromacct, int toacct){ // two locks
	... // Error checking
	balance[fromacct] -= amt;
	balance[toacct] += amt;
	... // Extra code
}

bool audit_all accounts() {
	// has to lock all accounts, the fuck
}

atomicity enforces isolation by having notion that we have actions at this level of threads are atomic
	e.g. they are either completed or not started yet
Critical sections are a way of implementing atomicity

Observability refers to what part of the system can the user see?
	If errors aren't seen, then who gives a shit?
Serializability - We take messy computation and pretend that it works in x sequence
	This means it works if we can dumb it down as such
	Bc we stupid, COMPUTERS Genius

Enforcing Critical sections
	Mutual exclusion
		When one thread works in a critical sections, exclude all other threads bc fuck them
	Bounded wait - if you attempt to access a critical section, do so in x time
		A thread should not starve waiting for a critical section
	The above are difficult with preemtion or multiple CPUs
		Preemtion  can be fixed with a one CPU setup
			Block signals pthread_signal // block timer interrupt is even better bc it solves it for ALL threads
				CS code
			Unblock signals pthread_signal 
			Can't work bc of signal handlers in today's stuff
		 
---

AD Chapter 28-33, 37
Locks are a variable and must be declared to be used
Locks either show avail or acquired
Threads acquire lock by running lock routine
POSIX Library uses Mutex as it's lock
	It is a wrapper and exits upon failure
Coarse-grained locking strategy - using different locks to access different data and data structures with different locks
Evaluate locks by checking 
	Basic task of mutex
	Fairness
	Performance
Spin-wait
	runs while(flag is up) do nothing
Test-and-set
	Check if thread is already working with that value
	Atomical checking procedure
	Higher overhead but allows better checking than spin-wait
	This results in a spin lock -> requres  preemtive scheduler
Evaluating this is based on correctness,  fairness and performance
Compare and swap instruction
	Test the value of at swap is same as expected
Load-linked and store-conditional instructions can be used to build locks
	Load-linked runs like a typical load register
	Store-conditional cecks for updates to ptr and fails if not updated
Fetch and add 
	Atomically increments a value while returning old value at particular address
Ticket lock - uses a ticket and turn value to build locks
	Does fetc and add first
	Then value is considered as thread's turned
	check who's turn it is
Yield to get better CPU time usage by a thread
	Avoids spinning so much
	Yielding deschedules the thread
You can also sleep so that you don't leave too much to chance
	Scheduler controls too much of our previous approach
	Here, you can force a thread to sleep and wake it up later
futex_wait()
	Look at man page
two-phase lock 
	Spins for a little at the beginning and is put to sleep if it can't get it within that phase
Thread safe -> locking a data structire to make it useable by  threads
Approximate counter - representing single logical counter via numerous local physical counters as well as a global counter
Poor performance leads to accuracy
High performance leads to nonaccuracy
Hand-over-hand locking is to lock a node per list
	Threads then release and lock the next  node
For queues, there is a lock on the head and on the tail
	Add a dummy node to allow enqueue and dequeue instructions
Hash tables - lock per hhash bucket
	Enables many  concurrent instructions to take place
Condition variables
	explicit queue that threads can put themselves on when waiting for some state of execution
	You essentially wait on the variable and wake the thread when conditions make itself avail
Producer/consumer problem
	One or more producer threads and same with consumers
		Producers generate and put in buffer and consumers take and delete
	Race condition can mean that consumers take too much or too little
With condition var, always use  while loops
Semaphhore is an object we can manip with two routines
Scheduler states
	Run, ready, and sleep
Dining philosopher
	There are 5 phil
	1 fork between
	Each  uses 2 to eat
	Soln is to have each phil take left, but at least one of them needs to wait first
Concurrency bugs
	Non-deadlock bug
		Atomicity violation
			We may need certain things to happen first regardless of what is best for processing
		Order-violation
			Race conditions essentially
	Deadlock bugs
		Happens due to complex dependencies
		Encapsulation can happen due to modular development
Conditions for deadlock
	Mutex
		Threads claim exclusive control of resources that they require
		Literally just don't
		Fuck this that's why and fuck you for trying to understand this
	Hold-and-wait
		Threads hold resources allocated to them (e.g., locks that they have already acquired) while waiting for additional resources
		Acquire all locks at the same time
	No preemtion
		Resources (e.g., locks) cannot be forcibly removed from threads that are holding them
		pthread_mutex_trylock()
			Can cause livelock
				Two threads butting heads
				Add a random delay to fix
	Circular wait
		There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being requested by the next thread in the chain
		Total ordering can solve with two locks by requiring one is acquired before the other
		Partial ordering helps when other locks may exist
Scheduling can help with deadlock avoidance
Event-based concurrency
	Waiting for an event to occur and do the amount of work it requires
	Event loop is what happens when You get all events and process each one
select() or poll() -> look into this
	examines the I/O descriptor sets whose addresses are passed in readfds, writefds, and errorfds to see if some of their descriptors are ready for reading, are ready for writing, or have an exceptional condition pending, respectively. The first nfds descriptors are checked in each set, i.e., the descriptors from 0 through nfds-1 in the descriptor sets are examined. On return, select() replaces the given descriptor sets with subsets consisting of those descriptors that are ready for the requested operation. select() returns the total number of ready descriptors in all the sets
No locks are needed with event-based apps
	NO BLOCKING SYSTEM CALLS -> BIG NONO BC FUCKER WILL BLOCK THE DAMN FUCKING SERVER
Async I/O 
	enable an application to determine whether various I/Os have completed
State management
	More complex than traditional thread-based code
	Fix through continuation
		basically, record the needed information to finish processing this event in some data structure; when the event happens (i.e., when the disk I/O completes), look up the needed information and process the event
HDD
	Address spaces listed from sector 0 to n-1
	Write is atomic
	Torn write is when powerloss happens during write
	Disk has two platters, eachh of which is known as a surface
		Platter is where data can be stored
	Each surface is made of some hhard metal coated with thin magnetic layer
	Platters spin around a spindle
	Data is encoded in concentric circles of sectors known as tracks
	We need mechhanisms for us to sense magnetic patterns on the disk or write to them
	RW is done by disk head
		attached to disk arm that guides disk head
	Rotational delay happens when disk has to wait for 0
	Multiple tracks have a seeking time because of finding different tracks
	Cache is it's track buffer, where some small amount of memory for disk to hold 
Scheduling
	Shortest seek time first
		Orders queue by track and finds shortest seek time
		Nearest block first 
			Fixes SSTF issue of drive geom not avail for OS
		Starvation can happen 
			Think about it dummy, I'm not giving you the answer
	Elevator
		Runs back and forth by sweeps and processes as it passes by -> SCAN
		F-SCAN freezes queue sweeped when doing service
		C-SCAN sweeps only outer to inner and resets 
	Shortest positioning time first 
		Runs job based on relative time of seeking compared to rotation
		Seek faster, ok to go further

SK Chapter 5.2.5, 5.6, 6.1
Deadlock-locking between two locks and won't give up
Sequence coordination
	Lost notif problem
		Use eventcounts and sequencers to avoid
Literally repeats AD for 5.6
Bottlenecks
Capacity , utilization, overhead, useful work -> each service has
	Latency - delay  between change at in and corresponding change in out
	Throughhput - rate of useful work
		Like when I don't fucking procrastinate
Talks about stuff you already know, so skip 6.1

Deadlock Avoidance
There are situations where mutual 
	mutual exclusion is fundamental.
	hold and block are inevitable.
	preemption is unacceptable.
	the resource dependency networks are imponderable.
Consider when main mem is exitd, so we switch processes out to secondary but this involves creation of new I/O request descriptors, which must be allocated from main
	In this case, we exhhausted a critical resource
		similar situations are when processes free up resources upon completion or processes need more resource to complete
Failure of allocation request may be difficult to handle
sbrk(2) 
	Does not allocate more memory to the process
	Requests OS to change size of data segment in virtual process data space
	If we can determine that requested reservation would overtax mem, we can return error
		Process can decide what to do here
	If we wait until we realize no resource is avail, we would kill process
	Refusal can give us time to analyze resource consumption before it is too late
Over-booking 
	It is reletively save to grant more reservations than we have resources to fulfill, as clients tend to request max resource reservations simultaneously
	+ We can get more work done
	- We may not be able to do this gracefully
Dealing with rejection
	Sleep on it, no just do any of the following
		A simple program might log an error message and exit.
		A stubborn program might continue retrying the request (in hope tht the problem is transient).
		A more robust program might return errors for requsts that cannot be processed (for want of resources), but continue trying to serve new requests (in the hope that the problem is transient).
		A more civic-minded program might attempt to reduce its resource use (and therefore the number of requests it can serve)

Appendix I.6-I.10
SSD 
	Mapped to a binary mapping of Cells
		We can map SLC or MLC
Basic flash ops
	READ, WRITE, PROGRAM
		Program can change 1s to 0s
direct mapped FTL-a read to logical page N is mapped directly to a read of physical page N
log structured FTL-upon a write to logical block N, the device appends the write to the next free spot in the currently-being-written-to block; we call this style of writing logging

---

Synchronization and deadlock
	This area is a disaster 
		Default is unsynchronized
		Common bugs are races
			These are hard to debug
Avoid the need for synchronization
	0 Single threaded code
	1 Event-driven programming
		1 CPU, many  threads
		Common in IoT
		- forces restructuring of code
		- not parallel (multicore CPUs are not used)
			Gotten around by splitting up processes

while(true) {
	wait for an event E to arrive
	Every action is a critical section->act on E <- must be fast and cannot wait on anything, so no waitpid
													no read can be used
}

Synchronization via load and store
	1. Doesn't work for large objects
		char buf[8];
		T1: strcpy(buf, "foldero");
		T2: strcpy(buf, "alacaza");
		T3: char buf3[8]; strcpy(buf3, buf); // Picks up neither what T1 nor T2 wanna communicate
	2. Doesn't work for small objects
		struct s{unsigned int bita:1, bitb:1, bitc:1;};
		struct s v;
			T1: v.bita = 1;
			T2: v.bitb = 0;

movl v, %eax; orb $1, %eax; movl %eax, v
movl v, %eax; andb $253, %eax; movl %eax v

What is the right size? It depends on the architecture
	x86-64 4 byte and 8 byte quantities
		There is a catch where compilers optimize the damn code and eliminate var access as much as possible
			T1: for(i = 0; i < n; i++)
					...
			T2: n = ...;
			Will get rid of n variable and screw everything up
	Others may make things more difficult in the sense that no sizes work
		Ya know, fuck you

Volatile variables -> tells compilers to not get rid of certain variables

enum {BUFSIZE = 512};
struct pipe {
	char buf[BUFSIZE]; // Important to be power of two for our arithmetic
	unsigned r, w;
};

int readc(struct pipe *p) {
	if(p->r == p->w)
		return -1;
	return p->buf[p->r++ % BUFSIZE];
}

void writec(struct pipe *p, chhar c) {
	if(p->w == p->r == BUFSIZE)
		return;
	p->buf[p->w++ & (BUFSIZE-1)] = c;
}

---

// RACE CONDITION IN THAT WE HAVE A CHANCE OF THE LOCKS BEING OBTAINED AT THE SAME TIME
typedef lock_t;
void lock(lock_t *);
void unlock(lock_t *);

lock_l;
int readc(struct pipe *p) {
	lock(&l);
	int r = (p->r == p->w ? -1 : p->buf[p->r++]);
	unlock(&l);
	return r;
}

bool writec(struct pipe *p, char c) {
	lock(&l);
	bool r = p->w-p->r < BUFSIZE
	if(r)
		p->buf[p->w++ % BUFSIZE]=c;
	unlock(%l);
	return r;
} 

typedef int lock_t;

// Precondition is that we don't have the lock
void(lock lock_t *l) {
	while(*l == 1)
		continuel
		*l=1;
}

// Precondition is that we own the lock
void unlock(lock_t *l) {
	*l = 0;
}

Move a bottleneck outside of a critical section
	Locks are not enough

We call AMD, Intel, etc again

// Compare and swap
bool cas(int * p, int o, int new) {
	if(*p == o) {
		*p = n;
		return true;
	} else
		return false;
}

Shrink scope of lock
	Read vs write locks
	Granularity
	
Synchronization Cotd
	Blocking mutexes
	Condition Variables

typedef struct {
	bool acquired;
	thread_descriptor_t *blocked_list;
	lock_t lock;
} bmutex_t;

We maintain a linked list of thread descriptors
	Releasing a bmutex_t gives first thread lock, sets acquired to true

Implement thhe acquire primitive
void acquire(bmutex_t *b) {
	// lock the data structure
	again:
		lock(&b->lock);

		if(!b->acquired) { // no one has the lock
			b->acquired = 1;
			unlock(&b->lock);
		} else { // we have to yield essentially
			// we own the spinlock, so give it up for a bit
			// Someone else owns the mutex
			self->blocked = true; // Release primitive will set this false
			add_self_to_blocked_queue();
			unlock(&b->lock); // Release spinlock
			yield();
			goto again;

		}
}

void release(bmutex_t *b) {
	lock(&b->lock);
	b->acquired = 0;
	if(b->blocked != NULL)
		b->blocked_list->block=0);
	unlock(&b->lock);
}

Add a bit in td node if it is locked or not
	Tells whether or not the OS can run it
	Scheduler will not run these threads for those threads that are still blocked

Semaphores (E.W, Dijkstras)
	It is like a blocking mutex but it allows n threads to acquire a resource 
	Generalizes blocking mutexes to allow N threads to run simultaneously
	"locked" when counter is 0
	"unlocked" when counter is > 0
	0 <= counter <= N

P (allocate)
	acquire - You may wait
V (Release)
	elease - you can do whenever

Will blocking mutexes suffice to implement pipe (getc, putc)?
	No, it will wake you up whenever someone touches the pipe even to read when there is nothing to be written
	It is possible,  but it makes no sense since the pipe will chew up CPU time
	You could try to edit the kernel, but then what about other file types? 
		This would essentially complicate the kernel and make it overly complex to deal with

Condition Variable
	to use it, we need
		A boolean condition (in the designer's head) defining the variable
		A blocking mutex protecting the expression
			Don't change boolean except in critical section, whhere we mutex lock

API
wait(condvar_t *c, bmutex_t *b) {
	precondition; // b is acquired
	does; // Releases b and blocks until some other thread says the condition is true
	notify; // tells scheduler to wake up conditions waiting on the condition
	notify_all(condvar_t *c) // notifies all waiters on condition c
}

struct pipe {
	char buf[BUFSIZE];
	bmutex_t b;
	condvar_t noempty, nonfull;
}

pipe_readc(struct pipe *p) {
	again:
		acquire(&p->b);
		if(p->w == p->d) {
			wait(&p->c, &p->b);
			goto again;
		}
	char c = p->buf[p-<r++ % BUFSIZE];
	release(&p->b);
	notify(&p->nonfull);
	return c;
}

pipe writec() {
	p->buf[p->w++ % BUFSIZE];
	notify(&p->noempty);
}

Hardware Lock Elision
	We lock too much in a highly multithreaded environment and it bottlenecks
	Avoid most of the botllenecks caused by spinlock by exploiting hardware support for Intel TSX extension
	Assembly code 
		lock: movl $1, %eax
		again: 
			xacquire lock xchgl %eax, l -> Assumes jmp works
			cmp $0, %eax
			jnz again
			ret
		unlock
			xrelease mal $0, l
			ret

Deadlock cat
	scenario: speedup	sort | cat | tr -> sort | tr -->  sort | tail -n +3 | tr == (does some stuff and cat)
	new syscall that reads nbytes from infd to outfd
		while(copy(infd, outfd, nbytes))
			continuel
void copy(int infd, int outfd, int nbytes) {
	struct pipe *p1 = lookuppipe(infd);
	struct pipe *p2 = lookuppipe(outfd);
	lock(&p1->l);
	lock(&p2->l);
	memcpy(p2->buf, p1->buf, nbytes);
	unlock(&p1->l);
	unlock(&p2->l);
} // problem is when we have 2-directional pipes and two threads spinlock and shove data down a pipe which never gets used

Deadlock is a race condition
	4 classic conditions for  deadlock
		Circular wait
		Mutual exclusion
		No preemtion of locks
		Hold and wait
			A thread  can hold one lock while waiting on another

One solution to deadlock
	Dynamic deadlock detection
		OS, every time you want to lock, it looks for loops	
		Tells you the acquire fails
		if so,
			errno = EWOULDDEADLOCK
		App must deal with it
	Lock ordering
		Assume some total order on locks, typically their addresses
		Acquire locks in increasing order to let locks be prioritized
		Threads wait only on their first lock... if unavail -> startover
	Manual
		Kill threads that are dadlocked
			You fucking monster
				It's halloween tradition to dress up as monsters not act like one

Priority Inversion
	A kind of deadlock caused by having locks and priority scheduling
	We have 3 threads
		From thread lo, thread hi, and a thread mi where hi, lo, mi indicates priority level
	T lo is runnable and acquires blocking mutex, which runs fine
	T med is waiting
	T hi is waiting -> Runnable when lo acquires mutex
		We have a context switch where lo gives up CPU 
		Hi tries  to get the lock
	Now med is  runnable and has a lot of work to do
		Med prevents lo from doing work
		Lo cannot work->ergo, hi is shit outta luck and the station on Mars burns to shit
	Fixable via temp priority boosting so lo doesn't fuck over hi 

Receive Livelock
	Processing : Accepting 
	We have an interrupt routine
		Accepts packet and stores into buffer OR discards it
		If system gets too busy
			Input buffer fills up
		You want work getting done to be correlated with yourload, but you will have bottleneck bc fuck you this is real life
	occurs when two or more processes continually repeat the same interaction in response to changes in the other processes without doing any useful work. These processes are not in the waiting state, and they are running concurrently. This is different from a deadlock because in a deadlock all processes are in the waiting state.
	FIX by hi priority ISP if system free
	low priority ISP if system busy

Review of storage heirarchy
	Registers, ALU -> Only place to do computation
	Caching Levels
	RAM 
	Flash -> only available in new machines
	Disk -> sometimes left out
	Github -> He knows: data down here should be virtualized to be like ALU
^ Not exactly a pyramid when scaled correctly
	Fucking 3000BC era people -Eggert
We want to give the illusion of fast, large memory
	Reality is slow large and fast small
	So let's cheat like we always do

Google profile tool usage
Compile with -g
Find where you have pprof and libprofiler.o
LD_PRELOAD=~/lib/libprofiler.so CPUPROFILE=./rar.gperf my_prog my_argument
	#LD_PRELOAD: linker magic, overload existing function in my_prog
		Seed bank?
	#CPUPROFILE: gperf specific environment variable, tells gperf where to store the raw profiling data
my_prog is the program run->our c file or our executable?

---

AD Chapters 39, 40-41
Persistent storage -> hard disk drive or modern solid-state storage device stores information permenantly
File is a linear  array of bytes which you can read or write
Each file has a low-level name -> inode number
Directory is second low-level abstracting
Root is where directory starts
Very simple file system (vsfs)
Inode is the index node -> index in array where node is located
Fast file system is the fastest possible system where tracks are organized with the cylindrical structure

SK Chapters 2.5, 6.3.4
Programs can create files with a user-chosen name. read and write file's content and set and get a file's meta data
	Meta data include
		Time of last modif
		user ID and file's owner access permissions for other users
		Users can group files in directories to create naming network
		Grafting naming networks stored on a storage device ont an existing naming netwok allows for different devices to be incorporated into a single large naming network
UNIX Divide and Conquer
	Makes use of hidden layers of addresses on top of another to impement files
Naming layers of UNIX file sys
	Symbolic link layer
		Integrate multiple file systems with symbolic links
	Absolute path name layer
		Provide a root for the naming hierarchies.
	Path name layer
		Organize files into naming hierarchies.
	File name layer
		Provide human-oriented names for files.
	Inode number layer
		Provide machine-oriented names for files.
	File layer
		Organize blocks into files.
	Block layer
		Identify disk blocks

FAT Intro
BIOS is BASIC I/O Subsystem
BASIC is an intepreter 
BIOS ROM provides support for BASIC
DOS was never intendwed to provide features and performance of real timesharing systems
File systems structure
	Bootstrap - code to be loaded into mem and executed whhen computer is powered on
		MVS reserves the entire first track of the first cylinder for the bootstrap
	Volume descriptors - info for size, type, layout of file sys
		How to find other key meta-data descriptors
	File Descriptors
		Lists of blocks of currently unused space that can be allocated to files
	File name descriptors 
		Data structures thhat user chosen names with each file
DOS FAT file systems divide volume into fixed-sized blocks which are grouped into larger logical structure
First block of DOS FAT is bootstrap and Vol des info
File allocatin table (FAT) is next
Boot block BIOS parameter block and FDISK Table
	Seperate the first block from vol desc 
Most file systems seperate bootstrap from vol desc
	DOS combines into one
Format varies between partitioned hard disks and unpartition floppies between various DOS and Windows
Conceptually, Boot record
	begins with a branch instruction (to the start of the real boostrap code)
	followed by a volume description (BIOS Parameter Block)
	followed by the real bootstrap code
	followed by an optional disk partitioning table
	followed by a signature (for error checking)
After first few bytes of bootstrap comes BIOS Param block, which hhas brief summary of file system and device (and it's geom) and way file system is laid out
Device geom:
	number of bytes per (physical) sector
	number of sectors per track
	number of tracks per cylinder
	total number of sectors on the volume
way file system is laid out
	number of sectors per (logical) cluster
	the number of reserved sectors (not part of file system)
	the number of Alternate File Allocation Tables
	the number of entries in the root directory
Mircrosoft figured customers wanted to put multiple file systems on eachh disk as disks got larger
	Partitions smaller subdisks
FDISK tables are how they organized this
Per entry included:
	A partition type (e.g. Primary DOS partition, UNIX partition).
	An ACTIVE indication (is this the one we boot from).
	The disk address where that partition starts and ends.
	The number of sectors contained within that partition.
Disk partitioning changed structure of Boot Record
MBR is located in the first sector of the MBR
	This includes FDISK and Bootstrap that finds active partition and reads in Part. Boot record
	Most people except Gates make their MBR bootstrap ask what sys to boot from and boot active one by default
File descriptors are essentially directories
	DOS combines both file description and naming into a file descriptor
	DOS directory is a file that contains fixed directory entries
	Each entry (32-bytes) describes a single file
		an 11-byte name (8 characters of base name, plus a 3 character extension).
		a byte of attribute bits for the file, which include:
			Is this a file, or a sub-directory.
			Has this file changed since the last backup.
			Is this file hidden.
			Is this file read-only.
			Is this a system file.
			Does this entry describe a volume label.
		times and dates of creation and last modification, and date of last access.
		a pointer to the first logical block of the file. (This field is only 16 bits wide, and so when Microsoft introduced the FAT32 file system, they had to put the high order bits in a different part of the directory entry).
		the length (number of valid data bytes) in the file
DOS stores file modif times and dates as  a  pair of 16-bit no.
	7 bits of year, 4 bits of month, 5 bits of day of month
	5 bits of hour, 6 bits of minute, 5 bits of seconds (x2).
All time is relative to epoch->Jan 1, 1980-2017
Space is allocated in logical multi-block clusters
	No of clusters per block determined when file created
Larger chunk of space improves I/O  by reducing  no. of op to read or write
	Lower internal fragmentation
Max number of clusters vol  can  support depends on width of FAT entries
File directory entry contains pointer to first cluster of  that file
FAT entry tells us next block
EOF is -1
	We reserve 0 after -> cluster is free
	-2 is free cluster 
Older versions of FAT file systems did not free blocks when file deleted
	DOS Systems run out of space->garbage collection starts from root and finds every "valid" entry recursively until done and marked all clusters not in file system in FAT
	This allowed for recovery of  data
DOS FAT entries have been  widened to 16 and 32 bit
Extended file names were added through a reassociation  
Microsoft added alternate FATs as so it could backup the primary 
	Copied periodically
Everyone recognized single file system format importance when CDs were made available
ISO created formatting
the most ideomatic features of the DOS file system (the File Allocation Table) were irrelevent to a CDROM file system (which is written only once):
	We don't need to keep track of the free space on a CD ROM. We write each file contiguously, and the next file goes immidiately after the last one.
	Because files can be written contiguously, we don't need any "next block" pointers. All we need to know about a file is where its first block resides.
9660 directory entries, like DOS directory entries, contain:
	file name (within the current directory)
	file type (e.g. file or directory)
	location of the file's first block
	number of bytes contained in the file
	time and date of creation
They did, however, learn from DOS's mistakes:
	Realizing that new information would be added to directory entries over time, they made them variable length. Each directory entry begins with a length field (giving the number of bytes in this directory entry, and thus the number of bytes until the next directory entry).
	Recognizing the need to support long file names, they also made the file name field in each entry a variable length field.
	Recognizing that, over time, people would want to associate a wide range of attributes with files, they also created a variable length extened attributes section after the file name. Much of this section has been left unused, but they defined several new attributes for files:
		file owner
		owning group
		permissions
		creation, modification, effective, and expiration times
		record format, attributes, and length information

---

File systems
	Performance 
	Design

ORNL  (Ray  cluster star file system)
	1 EB 10^18  bytes 1000000TB
	10TB/s output
	40 Cabinets
	19^11 racks
	Disk is more native, more reliable
	Flash  is faster and  wears out
	Cheap per byte
		10TB/$140
		1TB/$110
	Lustre + ZFS
		ZFS
			Much like book file systems
			Local file system
		Lustre
			Distributed file systems
Performance metrics for I/O
	USER
		Throughput: total requests per second for whole file  system
			Read vs write combined
		Latency: delay between I/O  request and response
	BUILDER
		Utilization: Fraction of capacity doing useful work
I/O Performance strategies
	Small and fast down the pyramid to big and slow
		Like penises
	To speed this up, we exploit locality of reference
		Use them hips
		JK->hash the arrays of datas so that they are smaller

Locality of reference
	Spatial locality 
		Accessing a[i] means likely to access a[i+1]
	Temporal locality
		a[i] at time t likey to access a[i] at time t+1
Speculation
	OS guesses what IO will be done
	Prefectching: OS guesses app will read some block a[i] in the near future, so it caches  a[i] in smaller, faster memory
		If right, no halting problem here -> not completely fixed bc duh
		If wrong, extra overhead and may evict a file needed to be read later
	Batching: When fetching a[i], you also a[i+1], a[i+2], a[i-1], a[i-2]->essentially pick area around it
		If right, less comm overhead between higher and lower levels of the systems
	Dallying: When asked to write to a[i], OS delays and holds waits for you to write more data in neighborhood
		If you do, it batches the set of blocks, and does it at once

sync()->flushes all the data to permenant storage
	Tells the OS to stop fucking around and do it's requests
	OS will be busy so you should wait
	It flushes EVERYTHING
	Return tells you nothing
fsync(fol)->flushes and waits
	Takes milliseconds
	Not what database vendors want
		Picky bastards
fdatasync(fd)->does fsync BUUUUTTT also includes the metadata
	Metadata is the ls -l data

File system - large body of named data, some way  of partitioning it into sections so you know what is in it
	You need indices or directories to find sections
	Involves nonvolatile/persistent storage
	Daddy Eggert definition: data structure on primary and secondary storage for storing and finding block or stream data
	These are NOT databases
	They can be considered a subset of databases
	Database can implement this

A very simple file system
	Files start on sector boundaries
	Files are contiguous
	Last sector may be only partly used
	Directory at start (10 Sectors)
		Tells you where the files are
		Contains directory entries
	Creator specifies the size
	If first byte of file name is NULL
		Directory not in use
	To delete
		Change file name to NULL
	To create
		There must be a region of free space
	Eggert was not the first
		Most well known was RT11
		Others were fancy

FAT filesystem
	Performance sucks ass
	But its nice
	First block is empty and has MBR
		Boot sector
	Superblock is next and has the metadata for file system itself
		Version no. of file system
		Size of file system in blocks
	2 Region division
		FAT
			Table that tells you where files are
			Array of block numbers
			Originally 16-bits each
			In the ith entry
				0->EOF
				-1 (0xffff)->free
				n->index of the next block
				Collection of linked lists in disguise	
		Data Blocks
			4GiB eachh
			Files do not have to be put in here contiguously

Some files are directories
	Directories connect to other files
	Form a tree
	Root is called root -> Fucking wow
	Index of the root directory is in the Superblock
	Directorie entries
		Name (11-bytes)
		Size (inbytes)
		1st block

FAT is NOT a performance file system
	lseek() is shit here
	positioning ith byte takes walking through the entire file system
		O(N) op
	Random access sucks

Typical UNIX/Linux traditional file system
	Take blocks of underlying storage device
	Most is still data
		Pretty standard
	Beginning is MBR
	Superblock still works like FAT with a little more complexity
	Difference lies in the metadata
		Block bitmap is followed by inode table
	Block Bitmap
		1-bit per block
			Allocated data to tell where free blocks are
	Inodes
		1 per file
			Fixed size
			Contains metadata for that file
	Directory entry (classic)
		14-bytes for the name
		'\0' terminal
		2-bytes for inode no.
			0 if unused
	More recent versions 
		32-bit inode
		16-bit directory entry length
		8-bit name length
		8-bit file type
		23-byte file name
		Rest is file 
	Perks
		Avoid limits on file name lengths
		Various sizes allowed
		File type is  inode,  cached in direntry <- Directory only
	To delete a file, directory entries are simply extended from the length of the previous entry to entry deleted with some empty space

Saving grace here is that most people do not make database sized directory structures
	Bc our directory structure eats up memory

If you have an inode number, it tells you
Inode tells you
	Data struct
		Last modified time
		Owner
		Group
		Permission
		Block numbers
			Tells us what block no. to look at to see files->helps with performance
			Traditionally was 10 blocks
				Limits size of file representable
				8KiB*10 blocks
			UNIX gives us extra entry
				This points to block which gives us more block numbers -> that block is our indirect block
				1024 block addresses are in this block
					Indirection gives us 1033*8192
				There is double indirection when you point to block to block to data
					More indirection increases exponentially
						Double 1024^2 + 1024 + 10
							8GB<-so small still
				Each level of increased indirection increases number of blocks usable by 1000x roughly
1 Block is 8192B or 2^(13)B
32TB = 32 blocks
2^(30)B is GiB
2^(40)B is TiB
0 block index is free file 
Holey file -> file with basically a lot of 0s that we shouldn't bother to store
	Dumb form of compression
		If mostly 0s, file system can optimize by skipping these
Traditionally, inodes go double indirect
	Nowadays they go to triple indirect
	More likely they use a balance tree
inode tells you type,perm,size,user,group,lastMod but not the name
Directory is a file of type directories
	Dirents maps names to inode numbers
What can go wrong with the above inode?
	What is the name of the input file?
		struct stat st;
		if fstat(0, &st)
			error();
			//0->STDIN_FILENO
	find / -inum 273
		Very slow
		Could output more than one name
		Must be rude to do this
		Can output no names
To delete file (appl)
	Remove all dirents to inode
	Close all open fds for that file
To delete file (OS)
	Reference counts for files
		Add row for inodes for no. of directory entries that reference the inode
	Remove links open to that file
	Keep FDs up for reading till wiped
No loops in a directory hierarchy or Directed Acyclic Graphs that are not trees
	Makes a mess bc fuck you
		link("..", "d")
			.. parent
			. self
	If you think about it, it's nonsensical to have this anyways
To move files, make a copy and then delete  in original
File name resolution
	Permission needed
		open
		unlink
		link
		mkdir
		strings that name files
	Permission assumed
		close
		read
		write
		lseek
		dup2
EX Inode
	bin/grep->296/67
		Every slash resolves and jumps to that block first
	// regularly ignores one
		at beginning, superroot
	/ at end can be ignored or result error

Symbolic links
	- regular file
	d Directory
	l symlinks
	p fifo named pipe
	c char special file
	b block device
file where contents are a file name resolved via namei to an inode no.
namei
	searches recursively through symlinks
	Ironclad rule of expanding only x symlinks
		errno==ELOOP
rm removes links -> people can still access if race cond
chmod 700 still findable
shred overwrites the data 3 times with random data
	Overkill but fuck them
	still in disk blocks
shred at file system level but still doesn't work bc it is still backed up sometimes
only way to ensure this is to melt the disk drive

struct ext2_super_block {
	__u32 $_inodes_count; /* inodes count */
	__u32 $_blocks_count; /* blocks count */

	__u32 $_free_blocks_count; /* Free blocks count */
	__u32 $_free_inodes_count; /* Free inodes count */
	__u32 $_first_data_block;  /* First data block */
	__u32 $_log_block_size;    /* Block size */

	__u32 $_log_count          /* Log count */

	__u16 $_magic;             /* Magic signature */
}

---

AD Chapters 42-45, 13-17
FSCK - file system checker 
	Old method to overcome power failure save state
Journaling - write-ahead logging 
	Adds overhead but recovers quicker
Crash-consistency  problem - when we cannot copy  data over atomically
Fsck
	Only checkes if metadata is internally consistent
	Runs before file sys is mounted and made available
	On-disk should be made consistent and accessible to users
	Superblock check on reasonableness
	Free block check to understand which blocks are allocated in the file system
	Inode state check for corruption
	Inode link check verifies the link count of each allocated inode
	Duplicate check checks for inodes reporting to one block
	Bad blocks checks for outofrange block address
	Directory checks looks for directories  holding  specifically  formatted info on file system itself
Journaling
	When updating the disk, before overwriting the structures in place, first write down a little note (somewhere else on the disk, in a well-known location) describing what you are about to do
	Then write to a structure
		Logging
	If crashes, we lookback
Checkpointing
	Journal write
		Write the transaction, including a transaction-begin block, all pending data and metadata updates, and a transaction-end block, to the log; wait for these writes to complete
	Checkpoint
		Write the pending metadata and data updates to their final locations in the file system
			Doing so in one sequence may result in a race condition
Making sure of TxE atomicity -> must be written at the end AFTER all other writes
	Journal write
		Write the contents of the transaction (including TxB, metadata, and data) to the log; wait for these writes to complete
	Journal Commit
		Write the transaction commit block (containing TxE) to the log; wait for write to complete; transaction is said to be committed
	Checkpoint
		Write the contents of the update (metadata and data) to their final on-disk locations
Recovery 
	Skip update pending if crash in middle of write
	If crash happens after write, we will scan log and look for commits
		Transactions are replayed to write out final on-disk locations
			Redo logging
		File system can proceed from here on out
	Even during checkpointing
		Worst case is that we redo some ops
Journals are circular logs
	Journals should free up space when a transaction is checkpointed
		Journal write:Writethecontentsofthetransaction(containingTxB and the contents of the update) to the log; wait for these writes to complete.
		Journal commit: Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction is now committed.
		Checkpoint:Writethecontentsoftheupdatetotheirfinallocations within the file system.
		Free: Some time later, mark the transaction free in the journal by updating the journal superblock
Ordered journaling is nearly the same except user data is not written to the journal
Data journaling journals all user data as well
Order of data matters
Protocol for writing data blocks to disk befor metadata
	Data write: Write data to final location; wait for completion (the wait is optional; see below for details).
	Journal metadata write: Write the begin block and metadata to the log; wait for writes to complete.
	Journal commit: Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction (in- cluding data) is now committed.
	Checkpoint metadata: Write the contents of the metadata update to their final locations within the file system.
	Free: Later, mark the transaction free in journal superblock
Flash based SSDs run on cells
SLC flash is single bit in transistor
MLC flash two bits are encoded 
Triple level cell is three bits per cell
Checksum
	Takes chunk of data and summarizes
Address spaces
Mem API
	Stack nem
Address translation
Segemntation - CS 33 shit

SK Chapters  8.1.1, 8.2.1, 8.4.1, 9.2, 5.4, 6.2 Intro, 6.2.3-6.2.9
Fault is underlying defect, imperfection, or flaw that has potential to cause problems
Failure is crash
Software fault: A programming mistake, such as placing a less-than sign where there should be a less-than-or-equal sign. This fault may never have caused any trouble because the combination of events that requires the equality case to be handled correctly has not yet occurred. Or, perhaps it is the reason that the system crashes twice a day. If so, those crashes are failures. 
Hardware fault: A gate whose output is stuck at the value ZERO. Until something depends on the gate correctly producing the output value ONE, nothing goes wrong. If you publish a paper with an incorrect sum that was calculated by this gate, a failure has occurred. Furthermore, the paper now contains a fault that may lead some reader to do something that causes a failure elsewhere.
Design fault: A miscalculation that has led to installing too little memory in a telephone switch. It may be months or years until the first time that the presented load is great enough that the switch actually begins failing to accept calls that its specification says it should be able to handle.
Implementation fault: Installing less memory than the design called for. In this case the failure may be identical to the one in the previous example of a design fault, but the fault itself is different.
Operations fault: The operator responsible for running the weekly payroll ran the payroll program twice last Friday. Even though the operator shredded the extra checks, this fault has probably filled the payroll database with errors such as wrong values for year-to-date tax payments.
Environment fault: Lightning strikes a power line, causing a voltage surge. The computer is still running, but a register that was being updated at that instant now has several bits in error. Environment faults come in all sizes, from bacteria contaminating ink-jet printer cartridges to a storm surge washing an entire building out to sea.
Wrong results are errors
Error containment
	Mask the error, so the higher-level subsystem does not realize that anything went wrong. One can think of failure as falling off a cliff and masking as a way of providing some separation from the edge.
	Detect and report the error at its interface, producing what is called a fail-fast design. Fail-fast subsystems simplify the job of detection and masking for the next higher-level subsystem. If a fail-fast module correctly reports that its output is questionable, it has actually met its specification, so it has not failed. (Fail-fast modules can still fail, for example by not noticing their own errors.)
	Immediately stop dead, thereby hoping to limit propagation of bad values, a technique known as fail-stop. Fail-stop subsystems require that the higher-level subsystem take some additional measure to discover the failure, for example by setting a timer and responding to its expiration. A problem with fail-stop design is that it can be difficult to distinguish a stopped subsystem from one that is merely running more slowly than expected. This problem is particularly acute in asynchronous systems.
	Do nothing, simply failing without warning. At the interface, the error may have contaminated any or all output values. (Informally called a “crash” or perhaps “failthud”.) 
High error latency allows other errors to happen
Time to failure is time process operates correctly
Time to repair is time it takes for failed component to be repaired
Availability is characterized by TTF/TTR
	If multiple, use mean times
Hamming distance is how many bits can be changed to make the next value
	Form of forward error checking
Atomicity is all-or-nothing
Overall system fault tolerance model.
	error-free operation: All work goes according to expectations. The user initiates actions such as adding events to the calendar and the system confirms the actions by displaying messages to the user.
	tolerated error: The user who has initiated an action notices that the system failed before it confirmed completion of the action and, when the system is operating again, checks to see whether or not it actually performed that action.
	untolerated error: The system fails without the user noticing, so the user does not realize that he or she should check or retry an action that the system may not have completed. 
The golden rule of atomicity
	Never modify the only copy! 
All or nothin atomicity has version control and commits
Virtualizing memory gives us addresses and address spaces
	Modularizes our memory
Page maps translated addresses
	The virtual memory manager translates the page number of the virtual address to a block number that holds that page by means of some mapping from page numbers to block numbers.
	The virtual memory manager computes the physical address by concatenating the block number with the byte offset from the original virtual address.


POSIX defect 672
Summary	
	0000672: Necessary step(s) to synchronize filename operations on disk
Description	
	POSIX documents a way of ensuring data is actually sync'ed on permanent storage through fsync(), fdatasync() and aio_fsync().

	This way, previously written data, and/or modified meta-data, are guaranteed to be actually protected against a reasonably unexpected situation (system crash, power outage ...)

	However, when dealing with file entry handling, such as:
	  * file creation (open(O_CREAT))
	  * file renaming (rename())
	  * symlinking (symlink())
	  * hard-linking (link())
	  * etc.
	there is no documented way to actually give the same guarantee.

	Some implementations (such as the Linux glibc) have a somewhat (badly) documented way:
	  * open the container directory in read-only (O_RDONLY)
	  * apply fsync() or fdatasync() on it

	Please refer to the "fsync()'ing a directory file descriptor" thread on the austin-group-l mailing list for insightful comments on this issue.

	Several points were discussed, and these (possibly not fully correct) observations were made:
	  * directory entries are not attributes of the files they point to, and can not expect to be synchronized [when fsync'ing the file]
	  * tracking relationship between directory entries and file descriptors would be cumbersome (a file may be hard-linked in another directory, then have its initial entry being deleted, for example, or renamed to another location)
	  * it is not clear whether a directory can be opened at all using open() (readdir() may be the only allowed interface), and what would be the open flags
	  * it is not clear what fsync() on a directory file descriptor would do
Desired Action	
	Clarify that file meta-data have no relationship with directory entry(ies) on the POSIX side.
	Clarify how synchronizing a filename operation can be achieved.
 
 ---

 File system robustness

 Multiple filesystems

Mount Tables
	Multiple filesystems but still mounted to seem as one filesystems
Name | fs it talks about
files are identified by inode numbers
	Each fs has its own inode table so we will have clashes
	To resolve, we need device numbers
	namei consults the mount table when resolving each name
We want flexibility to unmount  at any time if possible and remount elsewhere if we want
	but we cannot have hardlinks across file systems
Multiple types of filesystems 
	Boot fs is ext4
	USB Drive is FAT32
	Solution in Linux O.O Programming
		All in C
			Real hackers use C according to Eggert
Linux VFS 
	Struct task
		Struct file struct - file dscriptors
			Struct file -> Describes files
				Struct inode -> open or not bc we can do stuff to files without opening

Levels of a Linux filesystem (Higher levels are concatenations of lower levels)
	Symbolic links (namei)
	Absolute and relative filenames (namei)
	Filename components and directories
	Inodes
	Filesystems and mounts
	Partitions
	Blocks
	Sector/pages 

To understand Erectile dysfunction -er i mean performance issues, we need to explore all of this
	Omitted cases
		File system is not on your computer at all (Network filesystems)
		File systems that span partitions
		RAID (Fuck this fucker)
	Secure erasing 
		unlink("secret")
			Same as $rm secret
			Can be read if 
				Attacker has the file open
				Link count was greater than 1
			Solns
				Hardware (or software) encryptor to erasei discard key
				Overwirtes sectors that contain  file's data and metadata
					Shred -> overwrites the file 3 times
						Used to do 25 times
						Due to SSDs partly
						HDDs also a reason
						As density increases, tracing is easier
						Will be once later in 2020
						Assumes writes are in place
				Flash wears out 100K
				Flash has more data than it lets on
	Flash
		TRIM command makes a block vanish and writes zeros in big blocks and data in smaller blocks
			Slow 
		SECURE ERASE command (Whole disk)
Good FS perfomance
	Block level: Schedule I/O requests
SSTF: Shortest seek time first
	Look at all requests and find request closest to head
	Best throughput
	But Starvation
FCFS: first come first serve
	Do I/O requests in order they come in
	Fair 
	But not very efficient
Elevator algorithm: SSTF but only >h
	Just keep doing I/Os above h and then reverse
	Combination between throughput and fairness
	But people on the edges wait longer bc it comes later
Anticipatory scheduling
	Anticipate future requests by dallying for a little
File system robustness (in the event of a power failure)
	Terminology
		Failure - problem actually occurs at run time
		Fault - latent problem in HW or SW
		Error - in your head
	Goals
		Durability - data survives limited failures
		Atomicity - changes must be  made or not made
			No in-betweens
		Performance - Run efficiently
			No Erectile dysfunction pls

App: Text editor SAVE (10 blocks)
	GOLDEN RULE OF ATOMICITY
		Never overwrite only copy of your data
		Assume atomic write of a single block
			when not assuming
				Use 3 blocks and use best 2 of 3 
				#0 if all 3 disagree
Lampson-Sturgis model for storage device failures
	Storage writes may fail
	Storage write may corrupt another block
	Storage blocks may decay spontaneously
	A later read to detect a bad block
	Assume errors are rare
	Assume repairs can be done in time
	Assumption/goal
		2 Copies suffice

UNIX filesystem problems for robustness
	File system invariants needed for robustness
		Every block is used for at  most  one purpose
		All referenced blocks are properly initialized
		All referenced data blocks are marked used in bitmap
		All non-referenced data blocks are marked free in bitmap

Reads d's block into RAM
Reads inode 1597 into RAM
Reads e's block into RAM
Update both blocks in RAM
Write inode to flash/disk
Write e's inode block back to flash/disk
Write d's inode block back to flash/disk
Update l.c in ram back to 1
Write inode back to flash

State 1: l.c.=2, only 1 link (old)
State 2: l.c.=2, 2 links
State 3: l.c.=2, only 1 link (new)

later free file?
	Storage not reclaimed
User observes
	link(A,B) user wanted rename(A,B)
Storage leak

fsck: looks for
	Inodes not in any directory entry
		Clear them: free storage
	Inodes whose link counts are wrong

Commit reccord
	Assum e rename didn't happen until another (commit record) blockis written
	More writes
	More storage space 
Journal
	ext4 can do this
	log what you are planning on doing and do it again in case of power failure
	Normally, cell storage is nonvolatile
	Alternatively, cell storage is a cache of a journal

Write ahead log
	Log planned actions (∆J)
	Commit (∆J)
	Install new values (∆CD)
	Done (∆J)
	Revcover FIFO (L-R)
Write behind log 
	Log old values (∆J)
	Install new values (∆CD)
	Commit (∆J)
	Recover LIFO (R-L)

I/O Scheduling vs Robustness
	Lots o' write requests {(block no., data), (block no., data), (block no., data)}
		List also the dependencies and I/O Scheduler must respect this
	I/O schedulers can reorder these. not unlimited: can interchange a write of blokc #B with read or write of block #B
	BIG Opt But messes up robustness

Virtual Memory
	Free programmer from worrying about physical memory locations
	Prevents processes from accessing memory they shouldn't
	Lets processes/threads share memory
	Lets VM be bigger than physical

Alternatives to virtual memory
	Hire better programmers
	Enable subscript checking, null ptr checking, etc., in software
	Base+bound registers
		Simple: only 2 registers
		Assumes contiguous memoryu per process
		External fragmentation
		Each program must be relocated to run
	Segmented memory
		We want to take our addresses and partition them into 2 parts

cr3 is register pointing to page table
	Page table stores page table entries
	Can read from table entry to table entry
		Don't let write
Physical pages syscall
	mmap(void *addr,
		 size_t len,
		 int prot,
		 int flags,
		 int fd,
		 off_t offset)
	*addr - virtual address
	len - number of bytes to adjust
		Should be mult of page size
	prot - what do we want our permission bit to look like after we are done
	flags - another flag option
		FIXED - modify only address I give you
		PRIVATE - private to current process so no other processes can see
	fd - file descriptor so we know which file to 
	offset - offset within file so we do not have to read entire file

3b look for inconsistencies
	Data Block no errors
		Pointing to a free block
			Check if bitmap is free
		Out of bounds
			0 to no. of blocks -> done
		Connect to reserved blocks
			already done in code
		Duplicate blocks
			Look at slides for pseudocode
		Unreferenced
			Data block not referenced by any, but have it labelled allocated
			BFREE, Total blocks, reused blocks | blocks that are allocated

---

AD Chapters 10, 18-22, 48, 38, 49
Caches are based on locality
	Temporal locality is storing memory recently accessed
	Spatial locality is storing memory near some recently accessed memory
Cache coherence
	CPU Race condition
	Fix with bus snooping so each cache pays attention to mem updates by observing bus that connects them to main memory
Cache affinity
	A process buolds up a fair amount of state
Single queue multiprocessor scheduling is simplu reuse framework for single processor scheduling
Multi queue multiprocessor scheduling is when you follow round robin so that jobs that are in one queue are organized only in that queue
Translation lookaside buffer is a hardware cache of popular virtual to physical mem
Increasing speed done by making memory available as you need it
Policies
RAID
	RAID is a logical way of putting in an array of disks. Hard drives have limited speed, which often leads to a high failure rate. Redundancy provides a failsafe to this issue, which is what RAID does. RAID comes in different forms, each with differences in what they function. (See figure 3 in appendix A for different forms of RAID)(“(Almost) Everything You Need to Know About RAID.”)
	The RAID controller, as the name suggests, controls the RAID. It is a software that manages the HDDs or SSDs (based on what you use), and organizes them into on logical, functioning unit. The controller offers a level of abstraction between an operating system and the physical drives. A RAID controller presents groups to applications and operating systems as logical units for which data protection schemes can be defined. As the RAID controller can access multiple copies on multiple physical devices, it has the ability to improve performance and protect data.
	Types
		RAID 0 is taking any number of disks and striping data across all of them. This will greatly increase speeds, as you're reading and writing from multiple disks at a time. An individual file can then use the speed and capacity of all the drives of the array. The downside to RAID 0 though is that it is not redundant, the loss of any individual disk will cause complete data loss.
		RAID 1 is generally used with a pair of disks, though could be done with more, and would identically mirror/copy the data equally across all the drives in the array. The point of RAID 1 is primarily for redundancy, as you can completely lose a drive, but still stay up and running off the additional drive(s).
		RAID 5 requires the use of at least 3 drives (RAID 6 requires at least 4 drives) and will take the idea of RAID 0, striping the data across multiple drives to increase performance, but also adds the aspect of redundancy by distributing parity information across the disks. With RAID 5 you can lose one disk and with RAID 6 you can lose two disks and still maintain your operations and data.
		RAID 10 requires at least 4 drives and is a combination of RAID 1 (mirroring) and RAID 0 (striping), getting you both increased speed and redundancy. This is often the recommended RAID level if you're looking for speed and still require redundancy.
Embedded systems -> networking
	UDP/TCP/RPC

SK Chapters 4.2, 4.3, 4.5
Remote procedure call 
	Client/service interaction wherein each request is followed by a response
At-least-once RPC: If the client stub doesn’t receive a response within some specific time, the stub resends the request as many times as necessary until it receives a response from the service
At-most-once RPC: If the client stub doesn’t receive a response within some specific time, then the client stub returns an error to the caller, indicating that the service may or may not have processed the request
Exactly-once RPC: The general idea is that, if the RPC requesting transfer of $100 from account A to B produces a“no response”failure,the client stub sends a separate RPC request to the service to ask about the status of the request that got no response.This solution requires that both the client and the service stubs keep careful records of each remote procedure call request and response
Network file system grafts remote file system to the client's local file system

Health Monitoring
IDing Deadlock
	identify all of the blocked processes
	identify the resource on which each process is blocked
	identify the owner of each blocking resource
	determine whether or not the implied graph contains any loops
Formal deadlock detection is 
	difficult, inadequate, and does not tell us how to fix it
Knowing whether or not system makes progress
	by having an internal monitoring agent watch message traffic or a transaction log to determine whether or not work is continuing
	by having the service send periodic heart-beat messages to a health monitoring service.
	by having an external health monitoring service send periodic test requests to the service that is being monitored, and ascertain that they are being responded to correctly and in a timely fashion.
	Pros and cons
		heart beat messages can only tell us that the node and application are still up and running. They cannot tell us if the application is actually serving requests.
		an external health monitoring service can determine whether or not the monitored application is responding to requests. But this does not mean that some other requests have not been deadlocked or otherwise wedged.
		an internal monitoring agent might be able to monitor logs or statistics to determine that the service is processing requests at a reasonable rate (and perhaps even that no requests have been waiting too long). But if the internal monitoring agent fails, it may not be able to detect and report errors.
	Combining methods
		the first line of defense is an internal monitoring agent that closely watches key applications to detect failures and hangs.
		if the internal monitoring agent is responsible for sending heart-beats (or health status reports) to a central monitoring agent, a failure of the internal monitoring agent will be noticed by the central monitoring agent.
		an external test service that periodically generates test transactions provides an independent assessment that might include external factors (e.g. switches, load balancers, network connectivity) that would not be tested by the internal and central monitoring services.
Managed Recovery
	The software should be designed so that any process in the system can be killed and restarted at any time. When a process restarts, it should be able to reestablish communiction with the other processes and resume working with minimal disruption.
	The software should be designed to support multiple levels of restart. Examples might be:
		warm-start ... restore the last saved state (from a database or from information obtained from other processes) and resume service where we left off.
		cold-start ... ignore any saved state (which may be corrupted) and restart new operations from scratch.
	The software might also designed for progressively escalating scope of restarts:
		restart only a single process, and expect it to resync with the other processes when it comes back up.
		restart all of the software on a single node.
		restart a group of nodes, or the entire system.
False reports
	the best option would be for a failing process to detect its own problem, inform its partners, and shut-down cleanly.
	if the failure is detected by a missing heart-beat, it may be wise to wait until multiple heart-beat messages have been missed before declaring the process to have failed.
	in some cases, we might want to wait for multiple other processes/nodes to complain.
	Trade-off is suffering unnecessary service disruptions for too early and longer outage with longer times
Other managed restarts
	non-disruptive rolling upgrades ... if a system is capable of operating without some of its nodes, it is possible to achieve non-disruptive rolling software upgrades. We take nodes down, one-at-a-time, upgrade each to a new software release, and then reintegrate them into the service. There are two tricks associated with this:
		the new software must be up-wards compatible with the old software, so that new nodes can interoperate with old ones.
		if the rolling upgrade does not seem to be working, there needs to be an automatic fall-back option to return to the previous (working) release.
	prophylactic reboots ... It has long been observed that many software systems become slower and more error prone the longer they run. The most common problem is memory leaks, but there are other types of bugs that can cause software systems to degrade over time. The right solution is probably to find and fix the bugs ... but many organizations seem unable to do this. One popular alternative is to automatically restart every system at a regular interval (e.g. a few hours or days).
	If a system can continue operating in the face of node failures, it should be fairly simple to shut-down and restart nodes one at a time, on a regular schedule.

Appendix B
Virtual machine monitor/hypervisor
	Indirection to run another OS on the system
Limited direct execution
	Virtualizing the CPU via jumping address to first instruction
	Virtual machhines must perform machine switch
Privileged instructions become tricky
	Normally
		Execute ins
		syscall traps to OS
		Switch kenel
		Handle sys call and return from trap 
		Switch User
		Resume instruction
	Virtualized
		Same thing, but VMMs must record info and then forward to OS
		SysCall traps to OS
		Call OS trap handler
		OS trap handler decodes
		Return from trap
		When OS signal received, do real return
		Resume exec
		OS returns to supervisor mode
			Not User
Virtual memory
	Normally
		Load from memory: TLB miss: Trap
		OS TLB miss handler: Extract VPN from VA; Do page table lookup; If present and valid: get PFN, update TLB; Return from trap
	Virtualized 
		Load from mem TLB miss: Trap
		VMM TLB miss handler: Call into OS TLB handler (reducing privilege)
		OS TLB miss handler: Extract VPN from VA; Do page table lookup; If present and valid, get PFN, update TLB
		Trap handler: Unprivileged code trying to update the TLB; OS is trying to install VPN-to-PFN mapping; Update TLB instead with VPN-to-MFN (privileged); Jump back to OS (reducing privilege)
		Return from trap
		Trap handler: Unprivileged code trying to return from a trap; Return from trap
		Resume execution (@PC of instruction); Instruction is retried; Results in TLB hit

---

Virtual memory
	A little bit of distributed systems

void * mmap(void *addr, size_t len, int  prot, int flags, int fd, off_t offset);
	Special value on fail
	mmap can be called again with different permissions
		Possible failure depending on OS
int getpagesize(void);
	len%pagesize != 0 makes no sense
	being replaced by 'long sysconf(...)'

Permissions
	--x code
	rwx dynamically generated code (danger)
	rw- ordinary data (stack nowadays)
	r-x code
	r-- read-only data
	-w- write-only data
	--- useful for catching common addressing errors
	the executable is a high cost option
mmap lets you do dynamic linking more efficiently
/dev/zero is most  common read of this file returns all zeroes
	mmap allows allocation of this type of memory

page faulting
	lack permissions
	page entry is invalid
Hardware treats the fault like any other trap
Its operations
	1. Terminate theh process
	2. if signal(SIGSEGV, h)...)
		   plant stuff in users stack that makes it look like a call to h
		   ret (exit syscall) (change rep, esp)
		   (can be used to grow data structure)
	3. Kernel says "It was a valid access"
		   So it changes the page table entry to allow valid access
		   Then it resumes instruction
		   This option is taken when trying to simulate machine that is hhuge, virtual memory is greater than  physical mem or page is not in RAM so it  misses, which brings in page from flash, updates PTE, and set  othher  PTE  to be invalid since we  already ran out of physical memory
Problem:
	How to choose a victim? (Not that kind, you dimwit)
		Nobody is currently using the page
		Page hasn't changed since it  was read in
		Pick a page that won't be read in a while
Page replacement policy
	Reference string: sequence of page numbers accessed by the program
	Order of pref
		Oracle->LRU->FIFO
		LRU no kernel
			Put clock value in PTE hardware updates it
			Kernel (every now and then) marks all PTEs as being invalid
				Uses resulting page faults to maintain clock values in software
		FIFO
			keep table of when eachh page brought into RAM

int swapmap(int proc, int va) {
	return disk address or FAIL
}

void pfault(int va, int proc, int accessType) {
	if(swapmap(proc, va) == FAIL)
		kill(proc);
	else {
		int vicpa = vicproc->pmap(vicva);
		write vicpa to flash at  location swapmap(vicproc, vicva);
		vicproc->pmap(vicva) = FAULT;
		read vicpa from flash at location swapmap(proc, va);
		proc->pmap(va)=pa;
	}
}

Optimizations demand panging dirty bit
	Don't load any page into RAM until page fault occrs for it - Less wait time (best case: we need few pages | worst case: we  could have batchhed them but we didn't)
	(Empty  page stable @ start)
	bring in crt page (C startup routine)
	bring in  man page (for main subroutine)

Might be in Hardware or implement in software make page r-- even if it should be rw-
	0 page is identical to swap
	1 page has changed

What can go wrong with file systems
Failure mode:
	Bit flips every now and then due to external environmental factors
	ECC - in RAM, error correcting code
		Single bit flips detected
	Cosmic rays hit CPU, bit flips in CPU
		Need special CPU with EC in CPU
	Power loss and system crashes
		Clever algorithms
		lose data in RAM, but in persistent storage will be updated
		Transactions, FSCK and journaling
	Drive failures, dropping and damaging hardware
		How old is the drive?
		How many bad sectors?
		Bad sectors -> replacement sectors
		or whole device failures

Annulized failure rate (AFR)
USER ERROR
	rm * .o (the space between * and .o removes everything)
CONFIGURATION ERROR
	mistake made by operation stack configured OS incorrectly which causes the loss of data
Different area of problem: in flash drive
	Drive failures and sector failures

Solutions:
	1. log-structured file system
		Instead of storing logs and cell data in the same drive, we store them in different drives (e.g. store data on disk, store log on  a flash drive, a lot of IO to flash drive and bytes are  cheaper on disk)
	2. Make 2 copies rather than 1
		RAID - Redundant array of independent/inexpensive(obselete) disks
			Put N drives together and pretend its one big drive
			0 = Concatenate N drives
				Lay out data in a single array group
				Striping N drives
					Lay out data in a striped version (which means smaller pieces but greater performance in IO)
			1 = Pair two drives
				Copies of one another
				Each high-level write = 2 low level writes 
				Each high-level read = 1 low level read
				No single point of failure
			4 = uses a dedicated parity disk and block-level striping across multiple disks
				No single drive point of failure
			5 = + No single point of failure
				+ No hot spot
				- Hard to grow
					Unless your IT knows how to properly build their OS programs
						Which is very rare
						I don't know how to either, but I've configured before, see notes for your server

RAID assumption
	When a drive fails, it's repaired before the next drive fails
		Procedure
			Remove bad drive
			Install good drive
			Rebuild contents (as you do regular I/O)

Distributed systems and Remote procedure calls (RPCs)
	Differ from ordinary calls
		+ Caller and callee do not share an address space (hard modularity)
		- No call by reference
		- Caller and callee might disagree about architecture 'long' / big endian vs little endian
			Marshalling/serializing/pickling 
				Sender
			Unmarshal/unserialize/unpickle
				Receiver
		Convention  for converting  data  structure to byte
		XML, say JSON | A bit slow | or IEEE 754  bit  pattern

RPC failure modes
	+ Neither side can  trash other side's data structure
	- Messages can get lost
	- Messages can get corrupted (Checksums) << Better checksum: if message corrupted, resend request response
	- Network might be down (or slow) <- Latency throughput << if no response, keep trying (at least once RPC, suitable for idempotent ops)/return error (at most-one RPC, suitable for transactions)/exactly-once RPC
	- Server might be down (or slow)

RPC performance issues
	Marshalling and unmarshalling CPU overhead
	Transmission delay

Several attacks on this problem
	Async calls
	Assumes call are indep
	complicates failure hadling
	response in  any  order

Cache (on the client) answers
Prefetch answers to questions likely to be asked

Lab 4A, 4B 
	Literally read the documentation and just do what they ask

The Challenges of Securing the Virtualized Environment
Virtualization propogates problems more
It is preferred bc of efficiency when you run more than one OS at a time
Hypervisor apps provide an emulated hardware device per VM
Virtualization lets users put numerous applications on a single device
VMs cannot run firewalls
Dynamic property of VMs  makes it difficult to move VMs
VMs migrated from one physical platform to another must be protcted and monitored
Antivirus only applies to file level, not other levels
Network protection is mostly just Software
	This does not provide protection for hypervisor itself
	Instead, a firewall should be used to protect hypervisor

---

SK Chapter 11-11.3
To achieve security, the system must obtain trustworthy answers to the following three questions before performing the requested action:
	1. Authenticity: Is the agent’s claimed identity authentic? (Or, is someone masquerading as the agent?)
	2. Integrity: Is this request actually the one the agent made? (Or, did someone tamper with it?)
	3. Authorization: Has a proper authority granted permission to this agent to perform this action?
Complete mediation
	For every requested action, check the above
Adversary - an entity that breaks into systems intentionally
We require that security can tolerate adversaries
	Problem is giving users x amount of authority
Security describes techniques that protect information and infor­ mation systems against unauthorized access or modification of information, whether in storage, processing, or transit, and against denial of service to authorized users
	Protection is security
Seperate mechanism from policy
	Privacy policy should be implemented, but also sometimes we need to be able to go in to work on stuff
Threat classifications
	Unauthorized information release: an unauthorized person cna read and take advantage of information stored in thhe computer or transmitted of networks
	Unauthorized information modification: an unauthorized person can make changes in stored information or modify messages that cross a network
	Unauthorized denial of use: an adversary can prevent an authorized user from reading or modifying information, even though the adversary may not be able to read or modify the information
Legitimate user acting as adversary is insider threat
Security techniques for dealing with threats
	making credit card information sent over the Internet unreadable by anyone other than the intended recipients,
	verifying the claimed identity of a user, whether local or across a network,
	labeling files with lists of authorized users,
	executing secure protocols for electronic voting or auctions,
	installing a router (in security jargon called a firewall) that filters traffic between a private network and a public network to make it more difficult for outsiders to attack the private network,
	shielding the computer to prevent interception and subsequent interpretation of electromagnetic radiation,
	locking the room containing the computer,
	certifying that the hardware and software are actually implemented as intended,
	providing users with configuration profiles to simplify configuration decisions with secure defaults,
	encouraging legitimate users to follow good security practices,
	monitoring the computer system, keeping logs to provide audit trails, and protecting the logs from tampering
Defense requires designer to know every attack possibility
Be explicit
	All assumptions should be reviewed
Design for iteration
	Assume errors will be made
Safety net
	Certification - allows designs to be verified that it matches the security policies
	Audit trails - allow logging for any malicious behavior to be looked at
	Feedback - find ways to get critique on failures
Security is a negative goal
Open design principle
	Let anyone comment -> all the help I can get
Minimize secrets 
	Truth prevails
Principle of least astonishment applies to mediation
	Make it transparent
Economy of mechanism
	Less complex -> more correct
Minimize common mechanism -> less unwanted comm paths
Fail-safe default -> make sure defaults do something safe
Least privilege principle -> organization is key to a safe security
Trusted modules make up a trusted computing base
	Building
		Specify security requirements for the TCB (e.g., secure communication over untrusted networks). The main reason for this step is to explicitly specify assumptions so that we can decide if the assumptions are credible. As part of the requirements, one also specifies the attacks against which the TCB is protected so that the security risks are assessable. By specifying what the TCB does and does not do, we know against which kinds of attacks we are protected and to which kinds we are vulnerable.
		Design a minimal TCB. Use good tools (such as authentication logic, which we will discuss in Section 11.5) to express the design.
		Implement the TCB. It is again important to use good tools. For example, buffer-overrun attacks can be avoided by using a language that checks array bounds.
		Run the TCB and try to break the security
Authenticating questions
	1. Who is this principal making the request? The guard must establish if the message indeed came from the principal that represents the real-world person “Alice.” More generally, the guard must establish the origin of the message.
	2. Is this request actually the one that Alice made? Or, for example, has an adversary modified the message? The guard must establish the integrity of the message.
Think of when you get Spam emails when differentiating seperating trust from authentication
	Trust is same name fine domain
	Authentication principle just lets it through should everything seem fine
Authentication principle steps
	1. A rendezvous step, in which a real-world person physically visits an authority that configures the guard. The authority checks the identity of the real-world person, creates a principal identifier for the person, and agrees on a method by which the guard can later identify the principal identifier for the person
	2. A verification of identity, which occurs at various later times. The sender presents a claimed principal identifier and the guard uses the agreed-upon method to verify the claimed principal identifier. If the guard is able to verify the claimed principal identifier, then the source is authenticated. If not, the guard disallows access and raises an alert.
A cryptographic hash function maps an arbitrary-sized array of bytes M to a fixed-length value V, and has the following properties:
	1. For a given input M, it is easy to compute V ← H(M), where H is the hash function; 
	2. It is difficult to compute M knowing only V;
	3. It is difficult to find another input M' such that H(M') = H(M);
	4. The computed value V is as short as possible, but long enough that H has a low probability of collision: the probability of two different inputs hashing to the same value V must be so low that one can neglect it in practice. A typical size for V is 160 to 256 bits.
Cryptographic hash functions, like most cryptographic functions, are computationally secure. They are designed in such a way that it is computationally infeasible to break them, rather than being impossible to break.
Computationally security is measured quantified using a work factor. For crypto­ graphic hash functions, the work factor is the minimum amount of work required to compute a message M' such that for a given M, H(M') = H(M). Work is measured in prim­ itive operations (e.g., processor cycles).
	If the work factor is many years, then for all practical purposes, the function is just as secure as an unbreakable one because in both cases there is probably an easier attack approach based on exploiting human fallibility.
Given that d(technology)/dt is so high in computer systems and cryptography is a fast developing field, it is good practice to consider the window of validity for a specific cryp­ tographic function. The window of validity of a cryptographic function is the minimum of the time-to-compromise of all of its components.
More generally, what we need is a protocol between the user and the service that has the following properties:
	1. it authenticates the principal to the guard; 
	2. it authenticates the service to the principal;
	3. the password never travels over the network so that adversaries cannot learn the password by eavesdropping on network traffic;
	4. the password is used only once per session so that the protocol exposes this secret as few times as possible. This has the additional advantage that the user must type the password only once per session.
Message authenticity requires both:
	data integrity: the message has not been changed since it was sent;
	origin authenticity: the claimed origin of the message, as learned by the receiver from the message content or from other information, is the actual origin.
Authentication and confidentiality. An application (e.g., electronic banking), might require both authentication and confidentiality of messages. This case is like a signed letter in a sealed envelope, which is appropriate if the content of the message (e.g., it contains personal financial information) must be protected and the origin of the message must be established (e.g., the user who owns the bank account).
Authentication only. An application, like DNS, might require just authentication for its announcements. This case is like a signed letter in an unsealed envelope. It is appropriate, for example, for a public announcement from the president of a company to its employees.
Confidentiality only. Requiring confidentially without authentication is uncommon. The value of a confidential message with an unverified origin is not great. This case is like a letter in a sealed envelope, but without a signature. If the guard has no idea who sent the letter, what level of confidence can the guard have in the content of the letter? 
Signing produces as output an authentication tag: a key-based cryptographic trans­ formation (usually shorter than the message)
The requirements for an authentication system with shared-secret key K are as follows:
	1. VERIFY (M', T', K) returns ACCEPT if M' = M, T' = SIGN (M, K)
	2. Without knowing K, it is difficult for an adversary to compute an M' and T' such
	that VERIFY (M', T', K) returns ACCEPT
	3. Knowing M, T, and the algorithms for SIGN and VERIFY doesn’t allow an adversary to compute K
For an adversary who doesn’t know key K, it should be impossible to construct a message M' and a T' different from M and T that verifies correctly using key K.
A corresponding set of properties must hold for public-key authentication systems:
	VERIFY (M', T', K2) returns ACCEPT if M' = M, T' = SIGN (M, K1)
	Without knowing K1, it is difficult for an adversary to compute an M' and T' such that VERIFY (M', T', K2) returns ACCEPT
	Knowing M, T, K2, and the algorithms for verify and sign doesn’t allow an adversary to compute K1

---

Networked file systems
Security  authentication

NFS
	Client
	Process
	----
	We have kernel under this
		VFS, talks to EXT4 or BTRFS if we like losing data
RPC via kernel
	Network goes down
		int n = read(fd, buf, 1024);
		if(n < 0) {
			errno == EINTR
				// Traditionally no
				// Yes ^<- both choices are shot
		}

NFS file handle
	Uniquely IDs a file on a server (Survives renaming the file) (lke inode # + de# +?)
	Persists even if server reboots
		(So that server keeps running if server crashes)
If's common to reuse inode # in ext4, etc.
	f=inode#2932
	unlink("f") // Removes file 2932
	fd = open("g", O_CREAT | O_RDWR,...);
	// Can be 2932

"Stateless server" -> no data other than cache lies in the server

For performance, client will cache recently cached NFS data
Coordinating caches of multiple clients
NFS does not sync read and write
	Assumed loose coupling read and write
If you close a file, all servers do extra work to make sure any remaining IOs get to work on file first
	Close does not return until responses come back
	Close can fail with EDQUOTA
"open" syncshronized as well like close

NFS Benchmark
	specbench.org
		SPEC SFS 2014
NFS has issue with security
	Original -> Assume client and server kernels are trustworthy
		Client process says read(fd,...)
		READ(fd, vid thhat's reading 1003 ? 0 (root))
	Server uses ordinary protection for UID  1003
	Root -> user does anything
	Nobody -> User does nothing
Clients disagree about userid <-> mapping

Security issue
	Iran DoS BBC
		Also jammed 2 sat feeds
	Supreme council on virtual space
		Blocks large fraction of websites in iron

Security
	Real world defends against force and fraud attacks
	Virtual world 
		Main form are attacks against 
			Privacy - release of information
			Integrtity - tamper with other people's data
			Service - DoS (like when you're tryna sleep with a 17-y/o and FBI stops you)
Allow authorized access (Positive goal)
Disallow unauthorized access (Negative goal)
Good performance (Positive goal)

Thread modeling + classification
	Insiders
	Social engineering
	Network attacks
		Virus/drive-by-download
		DoS
	Device Attacks
		USB
	etc.

General function needed for security mechanisms in OS
	Authentication
	 	Password, credentials
	Integrity
		Checksum
			Checksums of modifications are checked
	Authorization
		Permissions of a user 
		Access control list
	Auditing
		Allows skeptics to look at OS and feel assured
		Log
	---------------------
	Correctness
		Mistakes are fatal
	Performance
		People want your security to be quick
		
---

SK Chapter 11.4-11.8
Public key -> encrypts 
	Private key can only decrypt
Encryption holds off 
	Ciphertext-only attack
	Known-plaintext attack
	Chosen-plaintext attack
	Chosen-ciphertext attack
Encryption and signing assures confidentiality and authentication, respectively
Receiver
	1. verify hash: compute HASH(key, counter) and compare result with the one in message
	2. if hash verifies, then increment counter and open garage. If not, do nothing. 
authorization: This operation grants a principal permission to perform an operation on an object.
mediation: This operation checks whether or not a principal has permission to perform an operation on a particular object.
revocation: This decision removes a previously-granted permission from a principal. 

Adhikari et al 2019 Chapter 1-2, 6-8
Resources through high-speed network is called cloud
Workflow of cloud is in DAG
	Each DAG has weight
In a workflow model, each task is a node and each node has various other nodes it connects to
Master-slave vs parent-child?
Service pods run permenantlu
Job/batch pods help execute tasks and terminate on completion
Cloud can do without server
Execution models in workflow
	Queue model
	Direct executor model
	Bridge model
	Decentralized model
Fog computing

---

Authentication - Prevents masquerading 
				 External (from outside world) (Trusted login agent-program [OS], Passwords, ID Token-needs hardware, Biometrics, Multifactor authentication)
				 Internal (Inside the system) (Badge)
				 	 OS Has recorded your ID Somewhere
				 		 UNIX is in process descriptor 
				 		 UID is User that originally ran program
				 		 EUID is what is used for access checks
	Attacks on external authentication
		Surf for password
			Look over shoulder
			Camera spying
		Guess password
		Steal tokens
		Fraudulent servers
			Setting up a server that looks like your website
		Chain breaks
		Password databases
		Buggy login agents
		SIM Card attacks
		Default password
		Attack password recovery
		Bad routers

setuid program
$ ls -l /usr/bin/passwd
  rwsr-xr-x root (date) /usr/bin/passwd
  euid is that of the file's owner ^
  If s permission exists, its setgid
  Trusted by root to do the right thing
  Do the right thing with /etc/shadow etc.
  PATH=/my/bad/bin

Syscalls for interval authentication
	getuid()
	geteuid()
	setuid(n) change uid to n <- allowed if our uid == 0 => WE ARE SUPERUSER

General forms of authentication
	Based on
		Who the principal (original actor) is  - retinal scan
		Something the principal has -  smart card <= Student ID, which is bootstrap for Password
		Something the principal knows - password <= SEASNet Password

Authorization
	Enforcing its rules
		Direct access to your resources 
			Map into your address space  struct r * <- R is a resource
			Done via mmap /etc.
			+ High performance access
			- Resource can become corrupted as we have too much access <= people are idiots and should be treated  as such
			- Races <= Racism? No, dumb fuck; race conditions.
		Indirect access to your resources
			Issue service requests via handler/syscalls
			OS Checks each  access
			- Slower <= boohoo, bitch
			+ Avoid corruption
			+ Easier to revoke permissions

Access control lists

$ get facl .
  7 usr: rwx
  5 group: r-x
  5 other: r-x
  chmod 755

$ selfacl -m g:tas:rwx <= tas is a  group
$ get facl
groupitas: rwx

Associated with each file
	A good list of access rights
		group:tas:rwz
		user:gblock:rw-
	ACLs are under the control of file's owners

ACLs: your resources have metadata for specifying data
Capabilities: objects (tickets) granting access to other objects
	UNIX file descriptor is like a capability fd = open(...) <= fd is opened and passed around within a process
	Linux reference an OS table
		Take description (encrypted) and send to other processes
		Practically impossible to form
	
ACLs in large organizations
	Users  often have "too many" rights
	If they just run programs with all of those rights
	The programs become too powerful
		$ make install - change your grades

Role-based access control
	Users can assume roles
	Access rights are  associated with roles - SW admin

Authhorization: What can go wrong
	Setup services (ordinary users)
		Mistakes in ACLs
		Wrong policy
		Deliberate misconfiguration

Services by privileged users
	Wrong (obslete, buggy) trusted program
	Backup/restor screwup (eggert: 1100-backup)
	Give passwords to bad guys (eggert: 2000-restore)

Distributed/Network security
	An untrusted network
		How to make security?
			Encryption-based network protocol
			Kerchoff's design principle
				Minimize what needs to be kept secret
				Assume shared secret K (Subject to replay attack)
					A->B {I'm Alice.}^K
					B->A "OK"
				NONCE (Avoids replay attack)
					A->B "I'm Alice"
					B->A "Nonce"
					A->B {Nonce}^K
					B->A "Ok"
				HMAC Algorithim Hash Message Authentication Code
					One possibility: A->B  M || HMAC(M, K) <= M is msg and HMAC is checksum
						Call up our crypto friends // Why does this sound like a bad cartoon?
						And ask for cryptographically secure checksum function
						SHA <- SHA1, SHA2, SHA3
						SHA(B)=Returns a hash function knowing H does not let you compute B' that SHA(B')=H
						Terrible Hash function
							XOR everything
						HMAC(M,K) = SHA(K||M) <= GOOD bc attacker can't deduce K from M

Typical authentication (via SSH, etc.)
	Multiphase (public key encryption) <- key pair 
		First establishh connection via public keys and mark
		Use a device as a shhared secret key
		Try comm using that key

Catro P et al 
	Rise of servers Computing 
	CACM 62(12) 44-54
	2019-12

Cloud Computing 
	Infrastructure as a Service (SaaS) (IDE America's did this)
		Service provider gives you 
			VMs
			On a networks
			Runs a hypervisor on each physical machine
				Think like ESXi setup
				Host (Physical) vs guest machines (VM)
		You provide
			OS (via a bootable image)
			Configuration
			Apps
		You get control of your VM
		Enterprise-level IT infrastructure and software spending
			2/3rds of it is cloud computing
				Will not grow to 1
			Obstacles of cloud computing
				Security - IoT problems still arise
		Often overprovisioned
			Allocate, pay too much CPU, RAM
			Buy small and grow slowly
				Although true, misleading
				This allows overload -> fuckery happens at the worst times because fuck you
		What we want is elasticity
			Provisioning Issues
				Scalability
				Availability
					Your services keep running albeit run slower, on server failure
				Fault tolerance
					Fault tolerance is if a server goes down, users shouldn't notice
				Quality of Service (QoS)
					QA testing
					Uptime/Availability, latency, throughput
				QoS monitoring
	Platform as a Service (PaaS) (E-Rehab did this)
		Like IaaS, BUT provider provides more of the software stack
			They provide
				Kernel
				Web server
				etc.
			You provide how many specialized servers to get
			Less expertise neceassary
			Autoscale is still necessary
				A problem: PaaS is evolving
	Back-end as a Service (BaaS) (Mobile app companies use this)
		Goal is to build something even more than PaaS
		Framework that runs in two environments
			Mobile client
			Backend server
	Function as a Service (FaaS) (Omni Wallet does this)
		Most common form of service computing
		Function is the unit of computation
			Not every single function
			The function called is the unit of computation
				Reliability exists
		Executed in response to a trigger (e.g. HTTPS Request) Supplies argument to function
		This is time limited because people appreciate efficiency
			Seconds or minutes normally none over
		No persistent state
			External storage server for state or DB

Internet of Things (IoT) (Edge Computing)
	Billions of Devices
		More than one device per person
		Each one wants to network with your IoT application
	Fog computing
		Pun on cloud computing hahahaha
		Reserve central servers which might run on central service
			Big and far away
				Like long distance relationships
				Never work tho
		Smaller clouds
			Good latency
		Trade latency for size
			Smaller is better
				Note for ladies, hear me out

Serverless computing
	Hide server usage from devs
	Runs code on-demand  (in servers) - part of billing
	Automatically scaled
	Billed only for run-time (ideally but not bileld seperately for persistent data)

function main(params, context) { // Params is a JSON Obj containing input request
	return {payload: 'Hello, ' + params.name};
}

FaaS architecture 

Event execution
	Arrival
	Validation via 
		Authentication
		Authorization
		Check resource limits
	Enqueue
	Dispatch
	---in worker
	Allocate a container
	Copy function's code into container
	Execute the function
	Stop/Deallocate container
First 3 in worker are cold start
	Issues with cold start
		Hurts latency
		Stem cell containers
			Vanilla
			Flavored
				With some library
				Preinstalled
		Reuse warm containers
			Allows containers to be reused for different apps
Action and Trigger  model for FaaS
	Trigger supplies events
	Action event executes an action/function
	Single event can trigger multiple actions (parallel execution)
	An action can  create an event that triggers other action(s) (Serial execution)

Problems
	Debugging (Functions execution logs as a tool)
		Gives you tools to analyze the logs
	Fixing bottlenecks and tools to analyze them
		No more 'ps'!
	Atomicity?
		Some serverless gives us support
		Some don't bc fuck you
	At most once functions
	At-least once functions
	Refactoring
		Splitting one function to multiple functions to fit code
		Moving to different versions during code runtime
	Evolving tools

Trusted software
	Most programs aren't trusted in Linux
		They have only your permissions
	Some programs have exceptions
		/bin/login -rwsr-xr-x root -> Runs on first login (root permission -> setuid)
		/usr/bin/sudo -rwsr-xr-x root -> Force run a program but we must have uid of root as euid is root
	If any trusted program has a bug, we can subvert security
Trusted Computing Base
	We take all programs and filter into trusted or untrusted
		Trusted should be MINIMAL
			Check everything in this sectiong VERY carefully
			Should you fail, you fucked

Break into any system
	Take every trusted program
	Backdoors in login.c
		int main() {
			read(username);
			read(password);
			if(strcmp(user, "kenb") == 0)
				return 0; // Gets fired
			encrypt(password, salt, r);
			if(strcmp(r, encrypted_word) == 0);
				return 0;
			else
				print("Try again, loser!");
			return 1;
		}
	Through gcc.c
		int main() {
			if(strcmp(srcfile, "login.c") == 0)
				compile("modified_program");
			if(strcmp(srcfile, "gcc.c") == 0)
				compile(modified_gcc.c)
		} // Still caught but eh
Reflections on trusting trust
Actions to take
	Write our own compiler
		Ken can defend you
	Assembly out
		Ken can fool you and only output this
	Use trusted code
		You have to trust someone
		Be careful who you trust

1.	RAID
	a. False; mirroring requires to copy everything over, striping you take advantage of parallelism
	b. False; RAID 4 surviving after any fail is bogus
	c. True; RAID 4 and RAID 5 run backup arrays  
	d. False; RAID 4 writes go to parity disk, not any other disk
	e. False; RAID 4 is still used

2. 	Sec questions
	a. False; RSA algorithms work because prime numbers are pretty fucking rare
	b. False; This is quite bogus
	c. False; Less code is more secure -> Less audits
	d. False; SSH uses shared-secret cryptography
	e. False; It is not a value

3. 	NFS
	a. False; we are getting FD, not FH from open()
	b. False; Race cond. can arise from lab 2
	c. False; opposite is true
	d. True; that's what we have
	e. False; Caching is for speed, not validation 

4. 	Hardware/Virtualization
	a. True; Reuse process in CPU everytime
	b. False; If we run same OS, we have shared host
	c. True; We can have VMs with different page table sizes
	d. False; This is impossible
	e. False; Too much to print

5. 	FS
	a. False; fsck is fucking slow
	b. False; log-structured file systems are the basis of flash bc robustness
	c. False; bogus bullshit
	d. False; We don't sync to secondary data storage anyways they do the same shit
	e. True; logs and git maintain the history

6.	FS
	a. False; FAT32 does not do this
	b. False; FS doesn't work this way
	c. False; directories don't do this themselves -> OS does this
	d. True; openi helps ensure good file
	e. False; you can do the same with hard links

7. 	FS
	a. False; violates naming rule
	b. True; you have ../ and ./ 
	c. False; not link counts do this
	d. False; obvious
	e. False; Modify the date and time of OS

8. 	No files are being mod
	a. True; corruption in filesystem possible
	b. False; only hard links affect link count
	c. False; stat is deterministic
	d. True; you can do this and this would work fine
	e. False; not how readdir works

9. 	Scheduling
	a. False; elevators are not fair
	b. False; there are cases where the other way works
	c. False; batching does nothing for fairness
	d. False; not all cases
	e. True; with upgrade, we do not know file size

10. Threads
	a. False; T can hand off responsibility to another thread with spinlocks, not mutex
	b. False; if S is global, any thread can call sem_post
	c. False; why the fuck... but in all seriousness, race conditions can occur even with care
	d. False; same reason as a
	e. False; checking both conditions decreases efficiency and only one allows leeway to race cond

11. SSL-Keeps a secure connection
	If we use SSL in OS, it's retarded
	We have some key for SSL anyways which is stored in OS, so anyone with root access knows how to decrypt the data

12. clri 2 -> clears inode entry 2
	We can record partially
	Recover names impossible

13. Spin locks are bad with high contention meaning multiply accessed file
	Dividing an array decreases size so spin locks should be good
	Probability is lower
	Mutexes: it doesn't matter too much
	Dependency on operations? Only dependency of answer is when we need to access multiple parts of the array

14. Symlinks - files that point to another file
	To run command with ternary function
	Extend the namei functions
	Problems if program stops due to signal, holds process, or does not halt at all
	Sort of possible, but cannot cover all cases because of the above

15. a) Simpsh reads from files, so we do not have anything to read or write to because stdin and stdout does not exist in simpsh
	b) Default files to STDIN and STDOUT and STDERR

16. a) Problems arise in
	   Sys calls will not give you what you want
	       Big endian format requires a formatting to turn it into little endian back to big endian
	   We need to know about architecture so we can do register compilation
	       C compiler make take care  of it	
	   No hard answer here
	   I personally like No
	b) Main issue is translation here, go from there

17. pause pauses le CPU
	Spin lock sleeps for x cycles -> Checks for free lock -> If free, lock, else sleep again and repeat
	So pause and run xchgl and then pause again if not free

---

The rise of serverless computing
First off, I would like to add notes saying fuck Eggert for adding this on as an additional reading assignment 
Serverless computing is done without thought for servers
Cost is pay as you go
Elasticity
SaaS is all application and API based
The rest is  as Eggert was talking about